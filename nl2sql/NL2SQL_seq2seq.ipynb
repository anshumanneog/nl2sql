{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_q = \"\"\"{\"phase\":1,\"question\":\"Find the Distinct count of games for RPG genre\",\"sql\":{\"conds\":[[3,0,\"RPG\"]],\"sel\":4,\"agg\":6},\"table_id\":\"1-9132139-1\"}\n",
    "{\"phase\":1,\"question\":\"how many distinct games for RPG genre\",\"sql\":{\"conds\":[[3,0,\"RPG\"]],\"sel\":4,\"agg\":6},\"table_id\":\"1-9132139-1\"}\n",
    "{\"phase\":1,\"question\":\"find total games for FPA genre\",\"sql\":{\"conds\":[[3,0,\"RPG\"]],\"sel\":4,\"agg\":3},\"table_id\":\"1-9132139-1\"}\n",
    "{\"phase\":1,\"question\":\"find distinct number of games for RPG genre\",\"sql\":{\"conds\":[[3,0,\"RPG\"]],\"sel\":4,\"agg\":6},\"table_id\":\"1-9132139-1\"}\n",
    "{\"phase\":1,\"question\":\"FIND TOTAL primary playtime in seconds FOR GAME_NAME_ID = 123456\",\"sql\":{\"conds\":[[0,0,\"RPG\"]],\"sel\":23,\"agg\":4},\"table_id\":\"1-9132138-1\"}\n",
    "{\"phase\":1,\"question\":\"FIND TOTAL NUMBER OF ACCOUNTS WHO PLAYED ON DATE = 11/20/2018\",\"sql\":{\"conds\":[[26,0,\"11/20/2018\"]],\"sel\":20,\"agg\":3},\"table_id\":\"1-9132138-1\"}\n",
    "{\"phase\":1,\"question\":\"Find the Number of games under franchise 125690\",\"sql\":{\"conds\":[[1,0,\"125690\"]],\"sel\":4,\"agg\":3},\"table_id\":\"1-9132139-1\"}\n",
    "\n",
    "{\"phase\":1,\"question\":\"Find the number of distinct Title_ID for Horizon Zero Dawn\",\"sql\":{\"conds\":[[4,0,\"Horizon Zero Dawn\"]],\"sel\":0,\"agg\":6},\"table_id\":\"1-9132139-1\"}\"\"\".split('\\n')\n",
    "extra_col = \"\"\"{\"id\":\"1-9132139-1\",\"header\":['TITLE_ID','FRANCHISE_NAME_ID','FRANCHISE_NAME','GAME_NAME_ID','GAME_NAME','SUPER_TITLE_NAME_ID','SUPER_TITLE_NAME','GAME_GENRE_ID','GAME_GENRE','PARTY_TYPE_ID','PARTY_TYPE','GLOBAL_SERVICE_PROVIDER_ID','GLOBAL_SERVICE_PROVIDER_ID_ID','GLOBAL_SERVICE_PROVIDER_NAME','GLOBAL_TITLE_ID','GLOBAL_TITLE_ID_ID','GLOBAL_TITLE_NAME','PMT_SERVICE_PROVIDER_ID_ID','PMT_SERVICE_PROVIDER_ID','PMT_SERVICE_PROVIDER_NAME','PMT_SERVICE_PROVIDER_JP_NAME','TITLE_NAME','SALES_TITLE_ID','SALES_TITLE','SALES_FRANCHISE_ID','SALES_FRANCHISE','PMT_TITLE_NAME','PMT_TITLE_JP_NAME','DISK_TITLE_NAME','TMDB_TITLE_NAME','ADOBE_RECOMMENDATIONS_FRANCHISE_NAME_ID','ADOBE_RECOMMENDATIONS_FRANCHISE_NAME','ADOBE_RECOMMENDATIONS_TITLE_NAME_ID','ADOBE_RECOMMENDATIONS_TITLE_NAME','TITLE_TYPE','TITLE_TYPE_ID','TITLE_TYPE_LEVEL_2','TITLE_TYPE_LEVEL_2_ID','TITLE_TYPE_LEVEL_3','TITLE_TYPE_LEVEL_3_ID','PLUGIN_IND','PLUGIN_GROUP_ID','PLUGIN_GROUP','MOVE_EXCLUSIVE_IND','TITLE_ATTRIBUTE_OVERRIDE_SET','SCEA_TITLE_RELEASE_RHQ_DT','SCEE_TITLE_RELEASE_RHQ_DT','SCEJ_TITLE_RELEASE_RHQ_DT','SCEASIA_TITLE_RELEASE_RHQ_DT','SCEA_GAME_RELEASE_RHQ_DT','SCEE_GAME_RELEASE_RHQ_DT','SCEJ_GAME_RELEASE_RHQ_DT','SCEASIA_GAME_RELEASE_RHQ_DT','PMT_TITLE_NAME_LONG','PMT_TITLE_NAME_ORIGINAL','PMT_TITLE_NAME_CLEANSED_IND','UNITY_TITLE_NAME','UNITY_TITLE_NAME_ID','UNITY_PUBLISHING_MODEL','UNITY_PUBLISHING_MODEL_ID','TITLE_PUBLISHER_TYPE','TITLE_DISTRIBUTION_REGION','TITLE_TARGET_CONSOLE_GENERATION','TITLE_ORIGINAL_CONSOLE_GENERATION','TITLE_MEDIA_TYPE','GAME_MEDIA_TYPE','TITLE_CONTENT_SUB_TYPE','TITLE_CONTENT_TYPE','MULTI_DISC_IND','SOURCE_TITLE_ID','SOURCE_SYSTEM_ID','ETL_ID_INSERTED','ETL_ID_UPDATED']}\n",
    "{\"id\":\"1-9132138-1\",\"header\":['GAME_PLAY_SESSION_ID','SESSION_START_UTC_DTTM','SESSION_START_UTC_DT_ID','SESSION_START_RHQ_DT_ID','SESSION_START_LOCATION_DT_ID','SESSION_START_UTC_TIME_ID','SESSION_START_LOCATION_TIME_ID','DEVICE_ID','DEVICE_TYPE_ID','IP_GEO_LOCATION_ID','SESSION_START_ACCT_ID','SESSION_START_ACCT_SCE_REGION_ID','SESSION_START_ACCT_COUNTRY_ISO_CODE','TITLE_ID','MEDIA_ID','SESSION_TYPE_ID','APPLICATION_VERSION_ID','FIRMWARE_VERSION_ID','GAME_PLAY_SESSION_CHARACTERISTICS_ID','GAME_PLAY_CHARACTERISTICS_ID','PRIMARY_ACCT_ID','PRIMARY_ACCT_SCE_REGION_ID','PRIMARY_ACCT_COUNTRY_ISO_CODE','PRIMARY_SESSION_LENGTH_SECONDS','SESSION_LENGTH_SECONDS','PSN_ACCOUNTS','LOCAL_ACCOUNTS','CONNECTED_DS4S','SECONDARY_GAMEPLAY_IND','LOCAL_GAMEPLAY_IND','SESSION_LENGTH_DEFAULTED_IND','SOURCE_SYSTEM_ID','ETL_ID_INSERTED']}\"\"\".replace(\"\\'\", '\\\"').split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5686517859062761"
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "import keras\n",
    "import re\n",
    "import json, pandas as pd\n",
    "import numpy as np\n",
    "from lib.query import Query\n",
    "import torch\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences \n",
    "import torch \n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import f1_score, accuracy_score \n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "Query.agg_ops = ['', 'MAX', 'MIN', 'COUNT', 'SUM', 'AVG', 'COUNT_DISTINCT']\n",
    "class load_data:\n",
    "    def __init__():\n",
    "        pass\n",
    "    \n",
    "\n",
    "def inject_table_names(sql, table_cols):\n",
    "    \"\"\"\n",
    "    replace col<index> in `sql` query with the <column_name> from `table_cols`\n",
    "    \"\"\"\n",
    "    try:\n",
    "        regex = re.compile(r'\\scol(\\d+)')\n",
    "        return regex.sub(lambda x: ' ' + table_cols[int(x.string[x.start()+4: x.end()])],  str(sql))\n",
    "    except Exception as e:\n",
    "        print(e, sql, table_cols)\n",
    "#         pass\n",
    "    return None\n",
    "def clean_text(text):\n",
    "    return \n",
    "    text = re.sub('\\W+', ' ', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "def load_data(filepath_phrase, filepath_table):\n",
    "    \n",
    "    lines = open(filepath_phrase, 'r').readlines()\n",
    "    lines += extra_q\n",
    "    df_phrases = pd.DataFrame([json.loads(line) for line in lines])\n",
    "    lines = open(filepath_table, 'r').readlines()\n",
    "    lines +=extra_col\n",
    "    df_tables = pd.DataFrame([json.loads(line) for line in lines])\n",
    "    # join table data with sql data\n",
    "    df = pd.merge(df_phrases, df_tables[['id', 'header']], left_on='table_id', right_on='id')\n",
    "#     df= df[:10000]\n",
    "    df['query_temp'] = df.sql.apply(lambda data: Query.from_dict(d=data))\n",
    "    df['query'] = df.loc[:, ['query_temp', 'header']]\\\n",
    "        .apply(lambda row: inject_table_names(row[0], row[1]), axis=1)\n",
    "    df['agg'] = df['sql'].apply(lambda x: x['agg'])\n",
    "    df['sel_col'] = df['sql'].apply(lambda x: x['sel'])\n",
    "    \n",
    "    questions = df['question'].apply(clean_text).values\n",
    "    queries = df['query'].values\n",
    "    column_names = df['header'].values\n",
    "    agg = df['agg'].values\n",
    "    sel_col = df['sel_col'].values\n",
    "    return questions, queries, column_names, df, agg, sel_col \n",
    "\n",
    "train_questions, train_queries, train_column_names, df_train, train_agg, train_sel_col,  = load_data('data/train.jsonl', 'data/train.tables.jsonl')\n",
    "test_questions, test_queries, test_column_names, df_test, test_agg, test_sel_col = load_data('data/test.jsonl', 'data/test.tables.jsonl')\n",
    "dev_questions, dev_queries, dev_column_names, _, dev_agg, dev_sel_col = load_data('data/dev.jsonl', 'data/dev.tables.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['', 'MAX', 'MIN', 'COUNT', 'SUM', 'AVG', 'COUNT_DISTINCT'],\n",
       " ['=', '>', '<', 'OP'])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_ops = Query.agg_ops\n",
    "cond_ops = Query.cond_ops\n",
    "len(agg_ops)\n",
    "agg_ops, cond_ops\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sony_df['header'][5].find('SESSION_START_UTC_DTTM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 7), [1, 1, 1, 1])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_seq_index(masked):\n",
    "    seq = ''.join(map(str, masked))\n",
    "    cond_seq = max(re.findall('1+', seq), key=len)\n",
    "    start_index = seq.index(cond_seq)\n",
    "    end_index = start_index + len(cond_seq) - 1\n",
    "    return start_index, end_index\n",
    "x = [0, 1, 1, 0, 1, 1, 1, 1, 0, 0]\n",
    "find_seq_index(x), x[4:7+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[12, 21],\n",
       "       [None, None],\n",
       "       [12, 33]], dtype=object)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[12, 21], [None, None], [12,33]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([100.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.]),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, b = np.zeros(10), np.zeros(10)\n",
    "a[0]= 100\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/project/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  \n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-6862fc85738d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# y[]=1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "x = [[2, 4], [1, 1], [3, 5], [2, 2]]\n",
    "# x = ''.join(map(str,x))\n",
    "x= np.array(x)\n",
    "\n",
    "y = np.zeros((4, 7))\n",
    "\n",
    "# y[]=1\n",
    "# y[[[row, col] for row, col in enumerate(x[:, 0])]]=1\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=int64)"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/project/anaconda3/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "class DataTransformer:\n",
    "    \"\"\"\n",
    "    :param questions: list of questions\n",
    "    :param headers: list of list of column names corresponding to the questions list\n",
    "    :param sql_dicts: list of sql dictionary corresponding to the question list\n",
    "    :param ops: list of ordered conditional operations s.t. ops at index i corresponds to the condops in sql_dict\n",
    "    :param agg: list of aggeration operations s.t. agg at index i should correspond to the agg in sql_dict\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, questions, header, sql_dict, ops, agg,\n",
    "                 embedding_filepath='data/glove.42B.300d.txt',\n",
    "                 max_words_per_question_percentile=99, \n",
    "                 max_words_per_column_percentile=99,\n",
    "                 max_columns_per_table_percentile=100):\n",
    "        \n",
    "        self.ops = ops\n",
    "        self.agg = agg\n",
    "        self.n_ops = len(ops)\n",
    "        self.n_agg = len(agg)\n",
    "        \n",
    "        self.max_words_per_question_percentile = max_words_per_question_percentile\n",
    "        self.max_words_per_column_percentile = max_words_per_column_percentile\n",
    "        self.max_columns_per_table_percentile = max_columns_per_table_percentile\n",
    "        self.lmtzr = WordNetLemmatizer()\n",
    "        \n",
    "        self.embedding_filepath = embedding_filepath\n",
    "        self._fit(questions, header)\n",
    "        self._initilize_index_of_seq()\n",
    "        \n",
    "        self.states_index = {'start': 0,\n",
    "                            'agg': 1,\n",
    "                            'selcol': 2,\n",
    "                            'condcol': 3, \n",
    "                            'condop': 4, \n",
    "                            'condval': 5,\n",
    "                            'end': 6}\n",
    "        \n",
    "        self.question_padding = 'pre'\n",
    "        \n",
    "    \n",
    "    \n",
    "    def clean_text(self, text):\n",
    "#         doc = nlp(text)\n",
    "#         text = ' '.join([token.lemma_ for token in doc])\n",
    "#         text = re.sub('\\W+', ' ', text)\n",
    "        text = text.lower()\n",
    "        text = re.sub('\\W+', ' ', text)\n",
    "        text  = ' '.join(list(map(self.lmtzr.lemmatize, word_tokenize(text))))\n",
    "        return text\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_embedding_matrix(filepath, word_index):\n",
    "        embeddings_index = {}\n",
    "        EMBEDDING_DIM = int(re.findall(('(?<=\\.)\\d+(?=d)'), filepath)[0])\n",
    "        f = open(filepath)\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "        f.close()\n",
    "\n",
    "        print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "        embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "        for word, i in word_index.items():\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                # words not found in embedding index will be all-zeros.\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "            else:\n",
    "        #         print(\"Not Found in Embedding Matrix: \", word)\n",
    "                pass\n",
    "        return embedding_matrix\n",
    "    \n",
    "        \n",
    "    def _fit(self, questions, headers):\n",
    "    \n",
    "        self.tokenizer = Tokenizer()\n",
    "        self.tokenizer.fit_on_texts(list(map(self.clean_text, questions)) + [y for x in headers for y in x])\n",
    "#         self.tokenizer.fit_on_texts(list(questions) + [y for x in headers for y in x])\n",
    "        question_sequences = self.tokenizer.texts_to_sequences(questions)\n",
    "        columns_sequences = [self.tokenizer.texts_to_sequences(columns) for columns in headers]\n",
    "        \n",
    "        self.max_words_per_question = int(np.percentile(list(map(lambda x: len(x), question_sequences)),\n",
    "                                                             self.max_words_per_question_percentile))\n",
    "        \n",
    "        self.max_words_per_column = int(np.percentile([len(col) for col_list in columns_sequences for col in col_list],\n",
    "                                                       self.max_words_per_column_percentile))\n",
    "\n",
    "        self.max_columns_per_table = int(np.percentile([len(col_list) for col_list in columns_sequences],\n",
    "                                                       self.max_columns_per_table_percentile))\n",
    "        \n",
    "#         self.embedding_matrix = DataTransformer.get_embedding_matrix(self.embedding_filepath, self.tokenizer.word_index)\n",
    "        return self\n",
    "    \n",
    "    def transform_questions(self, questions):\n",
    "        \n",
    "        question_sequences = self.tokenizer.texts_to_sequences(list(map(self.clean_text, questions)))\n",
    "#         question_sequences = self.tokenizer.texts_to_sequences(list(questions))\n",
    "        question_sequences = pad_sequences(question_sequences, maxlen=self.max_words_per_question, padding='pre', truncating='post')\n",
    "        return question_sequences\n",
    "        \n",
    "    def transform_columns(self, columns):\n",
    "        columns_sequences = [self.tokenizer.texts_to_sequences(columns_per_table) for columns_per_table in columns]\n",
    "        columns_sequences = [pad_sequences(column_list, maxlen=self.max_words_per_column) for column_list in columns_sequences]\n",
    "        columns_sequences = DataTransformer.pad_columns(columns_sequences, maxlen=self.max_columns_per_table, truncating='post', padding='post')\n",
    "        return columns_sequences\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def ohe_agg(self, aggs):\n",
    "        #         sel_col_ohe = np.zeros(max_columns_per_table)\n",
    "        ohe_vector = np.zeros((len(aggs), self.n_agg))\n",
    "        ohe_vector[np.arange(len(aggs)), aggs] = 1\n",
    "        return ohe_vector\n",
    "\n",
    "\n",
    "    def ohe_column(self, columns):\n",
    "        ohe_vector = np.zeros((len(columns), self.max_columns_per_table))\n",
    "        ohe_vector[np.arange(len(columns)), columns] = 1\n",
    "        return ohe_vector\n",
    "\n",
    "\n",
    "    def ohe_ops(self, ops):\n",
    "        ohe_vector = np.zeros((len(ops), self.n_ops))\n",
    "        ohe_vector[np.arange(len(ops)), ops] = 1\n",
    "        return ohe_vector\n",
    "    \n",
    "    def _transform_conds(self, conditions):\n",
    "        conditions = list(map(lambda x: self.clean_text(str(x)), conditions))\n",
    "        return self.tokenizer.texts_to_sequences(conditions)\n",
    "    \n",
    "    \n",
    "    def get_condition_seq(self, conds, questions):\n",
    "        def find_start_end_index(masked):\n",
    "            seq = ''.join(map(str, masked))\n",
    "            ones_seq = re.findall('1+', seq)\n",
    "            if not ones_seq:\n",
    "                return None, None\n",
    "            cond_seq = max(ones_seq, key=len)\n",
    "            start_index = seq.index(cond_seq)\n",
    "            end_index = start_index + len(cond_seq) - 1\n",
    "            return start_index, end_index\n",
    "        \n",
    "#         zipped = zip(self._transform_conds(conds), self.transform_questions(questions))\n",
    "        masked = list(map(lambda cond, question: np.in1d(question, cond).astype(np.int),\n",
    "                   self._transform_conds(conds), self.transform_questions(questions)))\n",
    "#         print(conds, questions)\n",
    "        start_end_indices = np.array(list(map(find_start_end_index, masked)))\n",
    "        start_seq, end_seq = np.zeros((len(questions), self.max_words_per_question)), np.zeros((len(questions), self.max_words_per_question)) \n",
    "        \n",
    "        for row, col in enumerate(start_end_indices[:, 0]):\n",
    "            if col:\n",
    "                start_seq[row, col] = 1\n",
    "        for row, col in enumerate(start_end_indices[:, 1]):\n",
    "            if col:\n",
    "                end_seq[row, col] = 1\n",
    "        \n",
    "        return masked, start_seq, end_seq\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def pad_columns(list_of_list_cols, maxlen, dtype='int32', padding='post', truncating='post', value=0.0):\n",
    "        \n",
    "        def _pad_columns(sequences, maxlen, dtype, padding, truncating, value):\n",
    "            diff = maxlen - len(sequences)\n",
    "            len_of_each_seq = len(sequences[0])\n",
    "            if diff > 0:\n",
    "                if padding == 'pre':\n",
    "                    return np.full(shape=(diff, len_of_each_seq), fill_value=value, dtype=dtype).tolist() + list(sequences)\n",
    "                else:\n",
    "                    return list(sequences) + np.full(shape=(diff, len_of_each_seq), fill_value=value, dtype=dtype).tolist()\n",
    "            elif diff < 0:\n",
    "                if truncating == 'pre':\n",
    "                    return list(sequences[-diff:])            \n",
    "                else:\n",
    "                    return list(sequences[:maxlen])\n",
    "            return list(sequences)\n",
    "        \n",
    "        return np.array([_pad_columns(list_, maxlen, dtype, padding, truncating, value) for list_ in list_of_list_cols])\n",
    "    \n",
    "    def _initilize_index_of_seq(self):\n",
    "        \n",
    "        self.start_index = np.arange(0, 1)\n",
    "        \n",
    "        self.agg_index = np.arange(1, self.n_agg + 1)\n",
    "        \n",
    "        self.sel_index = np.arange(self.n_agg + 1, self.n_agg +\n",
    "                                   self.max_columns_per_table + 1)\n",
    "        self.concol_index = np.arange(self.n_agg +\n",
    "                                      self.max_columns_per_table + 1, self.n_agg +\n",
    "                                      self.max_columns_per_table +\n",
    "                                      self.max_columns_per_table + 1)\n",
    "        self.conops_index = np.arange(self.n_agg +\n",
    "                                      self.max_columns_per_table +\n",
    "                                      self.max_columns_per_table + 1, self.n_agg +\n",
    "                                      self.max_columns_per_table +\n",
    "                                      self.max_columns_per_table +\n",
    "                                      self.n_ops + 1)\n",
    "        self.condval_index = np.arange(self.n_agg +\n",
    "                                       self.max_columns_per_table +\n",
    "                                       self.max_columns_per_table +\n",
    "                                       self.n_ops + 1, self.n_agg +\n",
    "                                       self.max_columns_per_table +\n",
    "                                       self.max_columns_per_table +\n",
    "                                       self.n_ops +\n",
    "                                       self.max_words_per_question + 1)\n",
    "        self.end_index = np.arange(self.n_agg + \n",
    "                                    self.max_columns_per_table +\n",
    "                                    self.max_columns_per_table +\n",
    "                                    self.n_ops +\n",
    "                                    self.max_words_per_question + 1, \n",
    "                                    self.n_agg +\n",
    "                                    self.max_columns_per_table +\n",
    "                                    self.max_columns_per_table +\n",
    "                                    self.n_ops +\n",
    "                                    self.max_words_per_question + 2)\n",
    "        \n",
    "        self.action_index = {'start': self.start_index,\n",
    "                'agg': self.agg_index,\n",
    "                'selcol': self.sel_index,\n",
    "                'condcol': self.concol_index,\n",
    "                'condops': self.conops_index,\n",
    "                'condval': self.condval_index,\n",
    "                'end': self.end_index}\n",
    "        return self.action_index\n",
    "#         self.agg_index = np.arange(0, self.n_agg)\n",
    "#         self.sel_index = np.arange(self.n_agg, self.n_agg +\n",
    "#                                    self.max_columns_per_table)\n",
    "#         self.concol_index = np.arange(self.n_agg +\n",
    "#                                       self.max_columns_per_table, self.n_agg +\n",
    "#                                       self.max_columns_per_table +\n",
    "#                                       self.max_columns_per_table)\n",
    "#         self.conops_index = np.arange(self.n_agg +\n",
    "#                                       self.max_columns_per_table +\n",
    "#                                       self.max_columns_per_table, self.n_agg +\n",
    "#                                       self.max_columns_per_table +\n",
    "#                                       self.max_columns_per_table +\n",
    "#                                       self.n_ops)\n",
    "#         self.condval_index = np.arange(self.n_agg +\n",
    "#                                        self.max_columns_per_table +\n",
    "#                                        self.max_columns_per_table +\n",
    "#                                        self.n_ops, self.n_agg +\n",
    "#                                        self.max_columns_per_table +\n",
    "#                                        self.max_columns_per_table +\n",
    "#                                        self.n_ops +\n",
    "#                                        self.max_words_per_question)\n",
    "        \n",
    "        return self\n",
    "#     def tot_\n",
    "    \n",
    "    def reverse_label_sequence(self, seqs, threshold=0.5, questions=None):\n",
    "        \n",
    "        return list(map(lambda seq,\n",
    "                            question: [self._reverse_label_sequence(one_seq, question) for one_seq in seq],\n",
    "                        seqs,\n",
    "                       questions))\n",
    "    \n",
    "#     def _pad(self, seq, max_len, pre=True):\n",
    "    def _normalize_question_index(self, index, question_len):\n",
    "        \n",
    "        if self.question_padding == 'pre':\n",
    "            # this won't work on truncated question\n",
    "#             return list(map(lambda x: x - self.max_words_per_question + question_len, indices))\n",
    "            return index - self.max_words_per_question + question_len\n",
    "        if self.question_padding == 'post':\n",
    "            return index\n",
    "    \n",
    "    @staticmethod\n",
    "    def _remove_unconsecutive_indices(indices):\n",
    "        if indices and len(indices) > 1:\n",
    "            return [indices[i] for i in range(len(indices)-1) if indices[i] == indices[i+1] - 1] + [indices[-1]]\n",
    "        return indices\n",
    "            \n",
    "    def _reverse_label_sequence(self, seq, question=None):\n",
    "        \n",
    "    \n",
    "        seq = np.array(seq)    \n",
    "        index = np.argmax(seq)\n",
    "#         print(index)\n",
    "        if index in self.start_index:\n",
    "            return None, 0\n",
    "    \n",
    "        if index in self.agg_index:\n",
    "            agg_seq = seq[self.agg_index]\n",
    "            agg = np.argmax(agg_seq)\n",
    "            state = 1\n",
    "            return agg, state \n",
    "        \n",
    "        elif index in self.sel_index:\n",
    "            sel_seq = seq[self.sel_index]\n",
    "            sel = np.argmax(sel_seq)\n",
    "            state = 2\n",
    "            return sel, state\n",
    "        \n",
    "        elif index in self.concol_index:\n",
    "            condcol_seq = seq[self.concol_index]\n",
    "            condcol = np.argmax(condcol_seq)\n",
    "            state = 3\n",
    "            return condcol, state\n",
    "        \n",
    "        elif index in self.conops_index:\n",
    "            condop_seq = seq[self.conops_index]\n",
    "            condop = np.argmax(condop_seq)\n",
    "            state = 4\n",
    "            return condop, state\n",
    "        \n",
    "        elif index in self.condval_index:\n",
    "            condval_seq = seq[self.condval_index] \n",
    "            condval_index = np.argmax(condval_seq)\n",
    "            \n",
    "            condval = None\n",
    "            if question:\n",
    "                question_words = text_to_word_sequence(question,\n",
    "                                                    filters=self.tokenizer.filters,\n",
    "                                                    split=self.tokenizer.split,\n",
    "                                                    lower=self.tokenizer.lower)\n",
    "#                 print(condval_index, question_words)\n",
    "                \n",
    "                condval_index = self._normalize_question_index(condval_index, len(question_words))\n",
    "#                 print(condval_index, question_words)\n",
    "                condval = question_words[condval_index]\n",
    "                \n",
    "            \n",
    "            state = 5\n",
    "            return condval if condval else condval_index, state\n",
    "        \n",
    "        elif index in self.end_index:\n",
    "            return None, 6\n",
    "            \n",
    "#             condval = np.argwhere(condval_seq > threshold).squeeze()\n",
    "#             if question is not None:\n",
    "\n",
    "#                 question_words = text_to_word_sequence(question,\n",
    "#                                                     filters=self.tokenizer.filters,\n",
    "#                                                     split=self.tokenizer.split,\n",
    "#                                                     lower=self.tokenizer.lower)\n",
    "#                 if not hasattr(condval.tolist(), '__iter__'):\n",
    "#                     condval = [condval]\n",
    "# #                 print(question_words, condval, hasattr(condval, '__iter__'), list(condval))\n",
    "# #                 print(condval, question_words)\n",
    "#                 condval = self._normalize_question_index(condval, len(question_words))\n",
    "#                 condval = DataTransformer._remove_unconsecutive_indices(condval)\n",
    "\n",
    "#                 condvalwords = np.array(question_words)[condval]                \n",
    "#                 condval = ' '.join(condvalwords)\n",
    "                \n",
    "#             state = 5\n",
    "#             return condval, state\n",
    "\n",
    "\n",
    "        return\n",
    "\n",
    "    \n",
    "    def label_sequence(self, sqls, questions):\n",
    "        \"\"\"\n",
    "        iterate over list of sqls and questions\n",
    "        \"\"\"\n",
    "        seq = map(lambda sql, question: self._label_sequence(sql, question), sqls, questions)\n",
    "        return np.array(list(seq))\n",
    "    \n",
    "    \n",
    "    def _label_sequence(self, sql, question):\n",
    "        \"\"\"\n",
    "        1-0-1 iteration over a sql and the corresponding question\n",
    "        \"\"\"\n",
    "        def init_seq():\n",
    "            seq = np.zeros(self.n_agg +\n",
    "                                 self.max_columns_per_table +\n",
    "                                 self.max_columns_per_table +\n",
    "                                 self.n_ops +\n",
    "                                 self.max_words_per_question + 2)\n",
    "            return seq\n",
    "        \n",
    "        seq = []\n",
    "        start = init_seq()\n",
    "        end = init_seq()\n",
    "        \n",
    "        agg = init_seq()\n",
    "        sel = init_seq()\n",
    "        \n",
    "        start[self.start_index] = 1\n",
    "        end[self.end_index] = 1\n",
    "        agg[self.agg_index] = self.ohe_agg([sql['agg']])\n",
    "        sel[self.sel_index] = self.ohe_column([sql['sel']])\n",
    "\n",
    "        seq.extend([start, agg, sel])\n",
    "        \n",
    "        for condcol, condops, condval in sql['conds']:\n",
    "            condcol_seq = init_seq()\n",
    "            condops_seq = init_seq()\n",
    "            condval_start_seq = init_seq()\n",
    "            condval_end_seq = init_seq()\n",
    "            \n",
    "            condcol_seq[self.concol_index] = self.ohe_column([condcol])\n",
    "            condops_seq[self.conops_index] = self.ohe_ops([condops])\n",
    "#             _, start_index, end_index = self.get_condition_seq([condval], [question])\n",
    "            _, condval_start_seq[self.condval_index], condval_end_seq[self.condval_index] = self.get_condition_seq([condval], [question])\n",
    "            seq.extend([condcol_seq, condops_seq, condval_start_seq, condval_end_seq])\n",
    "#         return seq\n",
    "        seq.append(end)\n",
    "        return np.array(seq)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     def generate_static_actions(self, question, column):\n",
    "#         self.agg_actions = [[self.states_index['agg'], agg_idx] for agg_idx, _ in enumerate(len(self.agg_ops))]        \n",
    "#         self.cond_actions = [[self.states_index['condops'], cond_idx] for cond_idx, _ in enumerate(len(self.cond_ops))]\n",
    "#         self.start_action = [self.states_index['start'], ]\n",
    "#         self.end_action = [self.states_index['end'], ]\n",
    "    \n",
    "    \n",
    "   \n",
    "    \n",
    "            \n",
    "datatransformer = DataTransformer(df_train['question'].values, df_train['header'].values, df_train['sql'].values, cond_ops, agg_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_seq = datatransformer.label_sequence(df_train['sql'].values, df_train['question'].values)\n",
    "reverse_label = datatransformer.reverse_label_sequence(target_seq, questions=df_train['question'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(None, 0),\n",
       "  (0, 1),\n",
       "  (4, 2),\n",
       "  (3, 3),\n",
       "  (0, 4),\n",
       "  ('paul', 5),\n",
       "  ('baccanello', 5),\n",
       "  (None, 6)],\n",
       " 'Name the score for opponent in the final being paul baccanello')"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_label[-100], df_train['question'].values[-100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# datatransformer = pickle.load(open('data/datatransformer.pic', 'rb'))\n",
    "pickle.dump(datatransformer, open('data/datatransformerlemm.pic', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sony_df = pd.read_csv('nl2sql/Sony_test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = datatransformer.transform_questions(df_test['question'].values)\n",
    "test_columns_sequences = datatransformer.transform_columns(df_test['header'].values)\n",
    "test_column_ohe = datatransformer.ohe_column(test_sel_col)\n",
    "test_agg_ohe = datatransformer.ohe_agg(test_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test_sequences, test_columns_sequences, test_column_ohe, test_agg_ohe = pickle.load(open('data/test_data.pic', 'rb'))\n",
    "\n",
    "# pickle.dump([test_sequences, test_columns_sequences, test_column_ohe, test_agg_ohe], open('data/test_data.pic', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_sequences = datatransformer.transform_questions(sony_df['question'].values)\n",
    "\n",
    "\n",
    "# test_columns_sequences = datatransformer.transform_columns(sony_df['header'].values)\n",
    "\n",
    "\n",
    "# test_column_ohe = datatransformer.ohe_column(sony_df['sel_col'].values)\n",
    "\n",
    "\n",
    "# test_agg_ohe = datatransformer.ohe_agg(sony_df['agg'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_sequences = datatransformer.transform_questions(df_train['question'].values)\n",
    "\n",
    "train_columns_sequences = datatransformer.transform_columns(df_train['header'].values)\n",
    "\n",
    "train_column_ohe = datatransformer.ohe_column(train_sel_col)\n",
    "\n",
    "train_agg_ohe = datatransformer.ohe_agg(train_agg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_sequences, train_columns_sequences, train_column_ohe, train_agg_ohe = pickle.load(open('data/train_data.pic', 'rb'))\n",
    "\n",
    "\n",
    "# pickle.dump([train_sequences, train_columns_sequences, train_column_ohe, train_agg_ohe], open('data/train_data.pic', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_agg_ohe, test_column_ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "class TrainingDataSet(Dataset):\n",
    "  def __init__(self, X, y):\n",
    "      \n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "  def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n",
    "\n",
    "# device = torch.device('cuda: 0')\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "embedding_tensors = torch.tensor(datatransformer.embedding_matrix, device=device, dtype=torch.float) \n",
    "\n",
    "train_sequences_tensor = torch.tensor(train_sequences, device=device, dtype=torch.long, requires_grad=False)\n",
    "test_sequences_tensor = torch.tensor(test_sequences, device=device, dtype=torch.long, requires_grad=False)\n",
    "\n",
    "test_columns_sequences_tensor = torch.tensor(test_columns_sequences, device=device, dtype=torch.long, requires_grad=False)\n",
    "train_columns_sequences_tensor = torch.tensor(train_columns_sequences, device=device, dtype=torch.long, requires_grad=False)\n",
    "\n",
    "train_column_ohe_tensor = torch.tensor(train_column_ohe, device=device, dtype=torch.float, requires_grad=False)\n",
    "test_column_ohe_tensor = torch.tensor(test_column_ohe, device=device, dtype=torch.float, requires_grad=False)\n",
    "\n",
    "train_agg_ohe_tensor = torch.tensor(train_agg_ohe, device=device, dtype=torch.float, requires_grad=False)\n",
    "test_agg_ohe_tensor = torch.tensor(test_agg_ohe, device=device, dtype=torch.float, requires_grad=False)\n",
    "\n",
    "# dev_sequences_tensor = torch.tensor(dev_sequences,device=device, dtype=torch.long)\n",
    "# dev_agg_ohe_tensor = torch.tensor(dev_agg_ohe.todense(), device=device, dtype=torch.float)\n",
    "\n",
    "\n",
    "train_dataset = TrainingDataSet(list(zip(train_sequences_tensor, train_columns_sequences_tensor)),\n",
    "                                list(zip(train_column_ohe_tensor, train_agg_ohe_tensor)))\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64)\n",
    "\n",
    "test_dataset = TrainingDataSet(list(zip(test_sequences_tensor, test_columns_sequences_tensor)),\n",
    "                               list(zip(test_column_ohe_tensor, test_agg_ohe_tensor)))\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64)\n",
    "# embedding_tensors.shape, embedding_matrix.shape                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([15886, 26]),\n",
       " torch.Size([15886, 73]),\n",
       " torch.Size([56363, 26]),\n",
       " torch.Size([56363, 73]),\n",
       " (32321, 300))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sequences_tensor.shape, test_column_ohe_tensor.shape, train_sequences_tensor.shape, train_column_ohe_tensor.shape, datatransformer.embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda: 0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embedding_matrix,\n",
    "                 max_columns_per_table,\n",
    "                 max_words_per_question,\n",
    "                 n_lstm_cells=100,\n",
    "                 bidirectional=True,\n",
    "                 trainable_embedding=True, n_layers=1):\n",
    "\n",
    "        super(Encoder, self).__init__()\n",
    "        vocab = embedding_matrix.shape[0]\n",
    "        feature_dim = embedding_matrix.shape[1]\n",
    "\n",
    "        self.word_embedding = nn.Embedding(vocab, feature_dim, _weight=embedding_matrix)\n",
    "\n",
    "        self.word_embedding.weight.require_grad = trainable_embedding\n",
    "\n",
    "        self.column_encoder = nn.LSTM(feature_dim,\n",
    "                                      n_lstm_cells,\n",
    "                                      num_layers=n_layers,\n",
    "                                      bidirectional=bidirectional)\n",
    "\n",
    "        self.question_encoder = nn.LSTM(feature_dim,\n",
    "                                        n_lstm_cells,\n",
    "                                        num_layers=n_layers,\n",
    "                                        bidirectional=bidirectional)\n",
    "\n",
    "        self.attended_question_encoder = nn.LSTM(feature_dim + max_columns_per_table,\n",
    "                                                 n_lstm_cells,\n",
    "                                                 num_layers=n_layers,\n",
    "                                                 bidirectional=bidirectional)\n",
    "\n",
    "        self.attended_column_encoder = nn.LSTM(feature_dim + max_words_per_question,\n",
    "                                               n_lstm_cells,\n",
    "                                               num_layers=n_layers,\n",
    "                                               bidirectional=bidirectional)\n",
    "\n",
    "        self.n_layers = n_layers * 2 if bidirectional else n_layers\n",
    "\n",
    "        self.n_lstm_cells = n_lstm_cells\n",
    "\n",
    "        self.max_columns_per_table = max_columns_per_table\n",
    "        self.max_words_per_question = max_words_per_question\n",
    "\n",
    "        self.attention_1 = nn.Linear(self.n_layers * self.n_lstm_cells, (self.n_layers * self.n_lstm_cells) // 2)\n",
    "\n",
    "        self.attention_2 = nn.Linear((self.n_layers * self.n_lstm_cells) // 2, 1)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        h_0, c_0 = [torch.zeros((self.n_layers, batch_size, self.n_lstm_cells), require_grad=False)] * 2\n",
    "        return h_0, c_0\n",
    "\n",
    "    def self_attention(self, output):\n",
    "        #         output.shape() -> seq_len, batch_size, n_lstm_cells * n_layers\n",
    "        output = output.transpose(1, 0, 2)\n",
    "        atten_weights = self.attention_2(F.tanh(self.attention_1(output))).squeeze()\n",
    "        #         atten_weights.shape() -> batch_size, seq_len\n",
    "        atten_weights = F.softmax(atten_weights, dim=1)\n",
    "        #         atten_weights.shape() -> batch_size, seq_len\n",
    "        #         output.shape() -> batch_size, seq_len, n_lstm_cells * n_layers\n",
    "        attended_output = torch.bmm(output.transpose(0, 2, 1), atten_weights.unsqueeze(2))\n",
    "        #         attented_output.shape() -> batch_size, n_lstm_cells * n_layers\n",
    "        return attended_output\n",
    "\n",
    "    def alignment_attention(self, lstm_outputs, last_output):\n",
    "        #         lstm_outputs.shape -> (batch_size, seq_len, hidden_size * n_layers)\n",
    "        #         last_output.shape -> (batch_size, n_layers, hidden_size)\n",
    "        lstm_outputs = lstm_outputs.permute(2, 1, 0)[-self.n_lstm_cells:].permute(2, 1, 0)\n",
    "        #         lstm_outputs.shape -> (batch_size, seq_len, hidden_size)\n",
    "        similarity = torch.bmm(lstm_outputs, last_output.permute(1, 0, 2)[-1].unsqueeze(2))\n",
    "        #         similarity.shape -> (batch_size, seq_len, 1)\n",
    "\n",
    "        atten_weights = F.softmax(similarity.squeeze(2), dim=1)\n",
    "\n",
    "        attended = torch.bmm(atten_weights.unsqueeze(1), lstm_outputs).squeeze(2)\n",
    "        #         attended.shape -> (batch_size, hidden_size)\n",
    "\n",
    "        return attended\n",
    "\n",
    "    def encode_columns(self, columns):\n",
    "        batch_size, columns_per_table, words_per_column, embedding_dim = columns.shape\n",
    "        columns = columns.view(-1, words_per_column, embedding_dim).transpose(1, 0, 2)\n",
    "        output, (ht, ct) = self.column_encoder(columns)\n",
    "        # ht.shape -> n_layers, batch_size * columns_per_table, n_lstm_cells\n",
    "        # output.shape -> words_per_column, batch_size * columns_per_table, n_dir * n_lstm_cells/hidden_size\n",
    "\n",
    "        encoded_columns = output[-1].view(batch_size, columns_per_table, -1)\n",
    "        # encoded_columns = ht.permute(1, 0, 2)[-1].view(batch_size, columns_per_table, -1)\n",
    "        # encoded_columns.shape -> (batch_size, columns_per_table, hidden_size\n",
    "        return encoded_columns\n",
    "\n",
    "    #     def encode_questions(self, questions):\n",
    "    #         output, (ht, ct) = self.question_encoder(questions)\n",
    "\n",
    "    #         encoded_questions = self.alignment_attention(output.permute(1, 0, 2), ht.permute(1, 0, 2))\n",
    "\n",
    "    def cross_attention(self, output_questions, output_columns):\n",
    "        #         output_questions.shape -> (batch_size, seq_len, hidden_size)\n",
    "        #         output_columns.shape -> (batch_size, columns_per_table, hidden_size)\n",
    "        cross_attended = torch.bmm(output_questions, output_columns.permute(0, 2, 1))\n",
    "        #         cross_attended.shape -> (batch_size, seq_len, columns_per_table)\n",
    "        question_seq, column_seq = cross_attended, cross_attended.permute(0, 2, 1)\n",
    "        return question_seq, column_seq\n",
    "\n",
    "    def forward(self, questions, columns):\n",
    "        questions_output, _ = self.question_encoder(questions.permute(1, 0, 2))\n",
    "        questions_output = questions_output.permute(1, 0, 2)\n",
    "        columns_output = self.encode_columns(columns)\n",
    "        cross_attended_questions, cross_attended_columns = self.cross_attention(questions_output, columns_encoded)\n",
    "\n",
    "        questions_cross_attended = torch.cat((questions_output, cross_attended_questions), dim=2)\n",
    "        columns_cross_attended = torch.cat((columns_output, cross_attended_columns), dim=2)\n",
    "\n",
    "        questions_output, _ = self.attended_question_encoder(questions_cross_attended)\n",
    "\n",
    "        columns_output, _ = self.attended_column_encoder(columns_cross_attended)\n",
    "\n",
    "        # lstm_output.shape -> seq_len, batch_size, n_dir * hidden_size\n",
    "        # lstm_ht.shape -> n_layer * n_dir, batch_size, hidden_size\n",
    "\n",
    "        # transposing to batch_first.\n",
    "        questions_encoded = questions_output[-1]\n",
    "\n",
    "        return questions_output.permute(1, 0, 2), columns_output.permute(1, 0, 2), questions_encoded\n",
    "\n",
    "\n",
    "#         return questions_ht.permute(1, 0, 2), columns_ht.permute(1, 0, 2), columns_seq\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, n_lstm_cells,\n",
    "                 repr_dim,\n",
    "                 n_layers,\n",
    "                 op_seq_len,\n",
    "                 action_embedding_dim,\n",
    "                 bidirectional,\n",
    "                 agg_ops,\n",
    "                 cond_ops,\n",
    "                 actions,\n",
    "                 use_self_attention=False):\n",
    "        \"\"\"\n",
    "        repr_dim -> hidden_dim of the encoded questions/columns\n",
    "        op_seq_len -> max_length of the generated query\n",
    "\n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "        #         self.action_embedding = nn.Embedding(len(actions), embedding_size)\n",
    "        self.action_embedding = nn.Embedding(len(actions) + len(cond_ops) + len(agg_ops) - 2, embedding_size)\n",
    "\n",
    "        feature_dim = repr_dim + action_embedding_dim\n",
    "        #         feature_dim += embedding_size\n",
    "        self.decoder_lstm = nn.LSTM(feature_dim,\n",
    "                                    n_lstm_cells,\n",
    "                                    num_layers=n_layers,\n",
    "                                    bidirectional=bidirectional)\n",
    "\n",
    "        self.bilinear = nn.Bilinear(n_lstm_cells, repr_dim, 1)\n",
    "\n",
    "        self.feature_dim = feature_dim\n",
    "\n",
    "        self.n_layers = self.n_layers * 2 if bidirectional else self.n_layers\n",
    "\n",
    "        self.agg_ops = agg_ops\n",
    "\n",
    "        self.cond_ops = cond_ops\n",
    "\n",
    "        self.start_idx = torch.arange(0, 1, device=device, dtype=torch.long)\n",
    "        self.agg_idx = torch.arange(1, len(agg_ops) + 1, device=device, dtype=torch.long)\n",
    "        self.selcol_idx = torch.arange(len(agg_ops) + 1, len(agg_ops) + 2, device=device, dtype=torch.long)\n",
    "        self.condcol_idx = torch.arange(len(agg_ops) + 2, len(agg_ops) + 3, device=device, dtype=torch.long)\n",
    "        self.condop_idx = torch.arange(len(agg_ops) + 3, len(agg_ops) + 3 + len(cond_ops), device=device,\n",
    "                                       dtype=torch.long)\n",
    "        self.condval_idx = torch.arange(len(agg_ops) + 3 + len(cond_ops), len(agg_ops) + 3 + len(cond_ops) + 1,\n",
    "                                        device=device, dtype=torch.long)\n",
    "        self.end_idx = torch.arange(len(agg_ops) + 3 + len(cond_ops) + 1, len(agg_ops) + 3 + len(cond_ops) + 2,\n",
    "                                    device=device, dtype=torch.long)\n",
    "\n",
    "        self.embedding_size = action_embedding_dim\n",
    "        self.use_attention = use_self_attention\n",
    "        self.op_seq_len = op_seq_len\n",
    "\n",
    "    def generate_action_matrix(self, questions_encoded, columns_output, questions_output):\n",
    "        # cols_repr_vector.shape -> batch_size, max_cols_per_tables, encoding_length\n",
    "        # questions_repr_vector.shape -> batch_size, encoding_length\n",
    "        # questions_output -> batch_size, max_words_per_question, endoding_length\n",
    "\n",
    "        #         agg_idx = self.action_indices['agg']\n",
    "\n",
    "        #         selcol_idx = self.action_indices['selcol']\n",
    "\n",
    "        #         condcol_idx = self.action_indices['condcol']\n",
    "        #         condop_idx = self.action_indices['condop']\n",
    "        #         condval_idx = self.action_indices['condval']\n",
    "\n",
    "        #         end_idx = self.actions['end']\n",
    "\n",
    "        seq_len = questions_output.shape[-2]\n",
    "        col_len = columns_output.shape[-2]\n",
    "        hidden_size = questions_encoded.shape[-1]\n",
    "        batch_size = questions_encoded.shape[0]\n",
    "\n",
    "        # padding_zeros shape -> 1, hiddent_size of question/column encoding\n",
    "        padding_zeros = torch.zeros(1, hidden_size, device=device, requires_grad=False)\n",
    "\n",
    "        # start_matrix shape -> 1, hidden_size of action_embedding\n",
    "        start_vector = torch.cat((self.action_embedding(self.start_idx), padding_zeros), dim=1).repeat(batch_size, 1, 1)\n",
    "\n",
    "        # agg_matrix shape -> n_ops, hiddden_size\n",
    "        agg_vector = torch.cat((self.action_embedding(self.agg_idx),\n",
    "                                padding_zeros.repeat(self.agg_idx.shape[0], 1)), dim=1).repeat(batch_size, 1, 1)\n",
    "\n",
    "        #\n",
    "        selcol_vector = torch.cat((self.action_embedding(self.selcol_idx).repeat(batch_size, col_len, 1),\n",
    "                                   columns_output), dim=2)\n",
    "\n",
    "        concol_vector = torch.cat((self.action_embedding.weight(self.condcol_idx).repeat(batch_size, col_len, 1),\n",
    "                                   columns_output), dim=2)\n",
    "\n",
    "        condops_vector = torch.cat((self.action_embedding(self.condop_idx),\n",
    "                                    padding_zeros.repeat(self.condcol_idx.shape[0], 1)), dim=1).repeat(batch_size, 1, 1)\n",
    "\n",
    "        condval_vector = torch.cat((self.action_embedding(self.condval_idx).repeat(batch_size, seq_len, 1),\n",
    "                                    questions_output), dim=2)\n",
    "\n",
    "        end_vector = torch.cat((self.action_embedding(self.end_idx), padding_zeros),\n",
    "                               dim=1).repeat(batch_size, 1, 1)\n",
    "\n",
    "        all_actions_matrix = torch.cat([start_vector,\n",
    "                                        agg_vector,\n",
    "                                        selcol_vector,\n",
    "                                        concol_vector,\n",
    "                                        condops_vector,\n",
    "                                        condval_vector,\n",
    "                                        end_vector], dim=1)\n",
    "        # that was tough\n",
    "\n",
    "        return all_actions_matrix\n",
    "\n",
    "    #     def get_action_vector(self, output_seq, n_words_per_question, n_columns_per_table):\n",
    "    #         index = output_seq.argmax()\n",
    "\n",
    "    # #         {'start_idx' : np.arange(0, 1),\n",
    "    # #         'agg_idx':  np.arange(1, )}\n",
    "\n",
    "    #         # for start and agg index\n",
    "    #         if index < 1 + len(self.agg_ops):\n",
    "    #             return self.action_embedding.weight[index]\n",
    "\n",
    "    #         # selcol\n",
    "    #         elif index < 1 + len(self.agg_ops) + n_columns_per_table:\n",
    "    #             return self.action_embedding.weight[1 + len(self.agg_ops)]\n",
    "\n",
    "    #         # condcols\n",
    "    #         elif index <  1 + len(self.agg_ops) + 2 * n_columns_per_table:\n",
    "    #             index = 1 + len(self.agg_ops) + 1\n",
    "    #             return self.action_embedding.weight[index]\n",
    "\n",
    "    #         # condops\n",
    "    #         elif index <  1 + len(self.agg_ops) + 2 * n_columns_per_table + len(self.cond_ops):\n",
    "    #             index = index - (1 + len(self.agg_ops) + 2 * n_columns_per_table)  + (1 + len(self.agg_ops) + 2)\n",
    "    #             return self.action_embedding.weight[index]\n",
    "\n",
    "    #         # condval\n",
    "    #         elif index <  1 + len(self.agg_ops) + 2 * n_columns_per_table + len(self.cond_ops) + n_words_per_question:\n",
    "    #             index = 1 + len(self.agg_ops) + 2 + len(self.cond_ops)\n",
    "    #             return self.action_embedding.weight[index]\n",
    "\n",
    "    #         # end\n",
    "    #         elif index <  1 + len(self.agg_ops) + 2 * n_columns_per_table + len(self.cond_ops) + n_words_per_question + 1:\n",
    "    #             index =  1 + len(self.agg_ops) + 2 + len(self.cond_ops) + 1\n",
    "    #             return self.action_embedding.weight[index]\n",
    "\n",
    "    def global_attention(questions_output, target):\n",
    "        pass\n",
    "\n",
    "    def get_action_vector_from_output(self, output_seq, action_matrix):\n",
    "        \"\"\"\n",
    "        output_seq.shape -> batch_size, n_actions\n",
    "        action_matrix.shape -> batch_size, n_actions, hidden_size\n",
    "        \"\"\"\n",
    "        top_index = torch.argmax(output_seq, dim=1).detach()\n",
    "        actions = torch.cat([action_matrix[n_batch_index, action_index, :self.embedding_size].unsqueeze(0).detach()\n",
    "                             for n_batch_index, action_index in enumerate(top_index)], dim=0)\n",
    "\n",
    "        #         action_matrix.shape -> batch_size, self.embedding_size/action_embedding_dim\n",
    "        return actions\n",
    "\n",
    "    def forward_step(self, previous_action_vector, questions_encoded, previous_hidden, output_actions_matrix):\n",
    "        \"\"\"\n",
    "        takes the previous_state batch and predicts the next token\n",
    "        \"\"\"\n",
    "        # previous_action_vector.shape -> batch_size, action_embedding_dim\n",
    "        # questions_encoded.shape -> batch_size, hidden_dim\n",
    "        # output_actions_matrix.shape -> batch_size, n_possible_actions, hidden_size\n",
    "        #         question_encoded = self.question_encoder()\n",
    "\n",
    "        n_actions_outputs = output_actions_matrix.shape[-2]\n",
    "\n",
    "        decoder_ip = torch.cat((previous_action_vector, questions_encoded), dim=1)\n",
    "        _, hidden_states = self.decoder_lstm(decoder_ip, previous_hidden)\n",
    "        last_output = hidden_states[0][-1]\n",
    "        # last_output.shape -> batch_size, hidden_size/decoder_n_lstm_cells\n",
    "        last_output = last_output.unsqueeze(1).repeat(1, n_actions_outputs, 1)\n",
    "        # last_ouput.shape -> batch_size, n_actions_outputs, hidden_size\n",
    "\n",
    "        bilinear_output = self.bilinear(last_output, output_actions_matrix)\n",
    "        # bilinear_output.shape -> batch_size, n_actions_ouputs, 1\n",
    "        bilinear_output = bilinear_output.squeeze(2)\n",
    "\n",
    "        return torch.softmax(bilinear_output, dim=-1), hidden_states\n",
    "\n",
    "    def generate_hidden(self, batch_size):\n",
    "        n_layers = self.n_layers\n",
    "        h_0 = torch.zeros(n_layers, batch_size, self.lstm_cells, requires_grad=False, device=self.device)\n",
    "        c_0 = torch.zeros(n_layers, batch_size, self.lstm_cells, requires_grad=False, device=self.device)\n",
    "        return h_0, c_0\n",
    "\n",
    "    def forward(self, questions_encoded, questions_output, columns_output, teacher_forcing_ratio=0,\n",
    "                target_output_seq=None):\n",
    "        \"\"\"\n",
    "        questions_encoded.shape -> batch_size, hidden_size | question representation\n",
    "        columns_output_vector.shape -> batch_size, max_columns_per_table, hidden_size | column representation\n",
    "        questions_output.shape -> batch_size, max_word_per_table, hidden_size | question representation at word level, used for attention\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = questions_encoded.shape[0]\n",
    "        # Prediction for start not required, start from index = 1\n",
    "        previous_hidden = self.generate_hidden(batch_size)\n",
    "\n",
    "        # start action\n",
    "        previous_action = self.action_embedding.weight[0]\n",
    "        action_matrix = self.generate_action_matrix(questions_encoded, columns_output, questions_output)\n",
    "        output_seq_list = []\n",
    "\n",
    "        use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "        if not use_teacher_forcing:\n",
    "            for index in range(1, self.op_seq_len):\n",
    "                output_seq, previous_hidden = self.forward_step(previous_action, questions_encoded, previous_hidden,\n",
    "                                                                action_matrix)\n",
    "                previous_action = self.get_action_vector_from_output(output_seq, action_matrix)\n",
    "                output_seq_list.append(output_seq.unsqueeze(1))\n",
    "        else:\n",
    "            for index in range(1, self.op_seq_len):\n",
    "                output_seq, previous_hidden = self.forward_step(previous_action, questions_encoded, previous_hidden,\n",
    "                                                                action_matrix)\n",
    "                #                 output_seq.shape -> batch_size, op_seq_len\n",
    "                previous_action = target_output_seq[:, index - 1, :]\n",
    "                output_seq_list.append(output_seq.unsqueeze(1))\n",
    "\n",
    "        out_seqs = torch.cat(output_seq_list, dim=1)\n",
    "        # out_seqs.shape -> batch_size, op_seq_len, n_actions\n",
    "\n",
    "        return out_seqs\n",
    "\n",
    "\n",
    "class NL2SQL(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder, decoder, ):\n",
    "        super(NL2SQL, self).__init__()\n",
    "        self.decoder = decoder\n",
    "        self.encoder = encoder\n",
    "\n",
    "    @classmethod\n",
    "    def initialise_encoder_decoder_network(cls, encoder_word_embedding_matrix,\n",
    "                                           encoder_max_columns_per_table,\n",
    "                                           encoder_max_words_per_question,\n",
    "                                           encoder_n_lstm_cells,\n",
    "                                           encoder_bidirectional,\n",
    "                                           encoder_trainable_embedding,\n",
    "                                           encoder_n_layers,\n",
    "                                           decoder_n_lstm_cells,\n",
    "                                           decoder_n_layers,\n",
    "                                           decoder_op_seq_len,\n",
    "                                           decoder_action_embedding_dim,\n",
    "                                           decoder_bidirectional,\n",
    "                                           decoder_agg_ops,\n",
    "                                           decoder_cond_ops,\n",
    "                                           decoder_actions, ):\n",
    "        encoder = Encoder(embedding_matrix=encoder_word_embedding_matrix,\n",
    "                          max_columns_per_table=encoder_max_columns_per_table,\n",
    "                          max_words_per_question=encoder_max_words_per_question,\n",
    "                          n_lstm_cells=encoder_n_lstm_cells,\n",
    "                          bidirectional=encoder_bidirectional,\n",
    "                          trainable_embedding=encoder_trainable_embedding,\n",
    "                          n_layers=encoder_n_layers)\n",
    "        decoder = Decoder(n_lstm_cells=decoder_n_lstm_cells,\n",
    "                          repr_dim=encoder_n_lstm_cells * 2 if encoder_bidirectional else 1,\n",
    "                          n_layers=decoder_n_layers,\n",
    "                          op_seq_len=decoder_op_seq_len,\n",
    "                          action_embedding_dim=decoder_action_embedding_dim,\n",
    "                          bidirectional=decoder_bidirectional,\n",
    "                          agg_ops=decoder_agg_ops,\n",
    "                          cond_ops=decoder_cond_ops,\n",
    "                          actions=decoder_actions,\n",
    "                          use_self_attention=False)\n",
    "\n",
    "        return cls(encoder, decoder)\n",
    "\n",
    "    def flatten_parameters(self):\n",
    "        self.encoder.flatten_parameters()\n",
    "        self.decoder.rnn.flatten_parameters()\n",
    "\n",
    "    def forward(self, questions, columns, teacher_forcing_ratio=0., target_output_seq=None):\n",
    "        questions_output, columns_output, questions_encoded = self.encoder(questions, columns)\n",
    "        out_seqs = self.decoder(questions_encoded, questions_output,\n",
    "                                columns_output, teacher_forcing_ratio, target_output_seq)\n",
    "        return out_seqs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NL2SQL.initialise_encoder_decoder_network(encoder_word_embedding_matrix,\n",
    "                                           encoder_max_columns_per_table,\n",
    "                                           encoder_max_words_per_question,\n",
    "                                           encoder_n_lstm_cells,\n",
    "                                           encoder_bidirectional,\n",
    "                                           encoder_trainable_embedding,\n",
    "                                           encoder_n_layers,\n",
    "                                           decoder_n_lstm_cells,\n",
    "                                           decoder_n_layers,\n",
    "                                           decoder_op_seq_len,\n",
    "                                           decoder_action_embedding_dim,\n",
    "                                           decoder_bidirectional,\n",
    "                                           decoder_agg_ops,\n",
    "                                           decoder_cond_ops,\n",
    "                                           decoder_actions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "optim = torch.optim.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelectPredictor(\n",
       "  (word_embedding): Embedding(32321, 300)\n",
       "  (word_embedding_agg): Embedding(32321, 300)\n",
       "  (question_encoder): LSTM(300, 150, dropout=0.3, bidirectional=True)\n",
       "  (question_encoder_agg): LSTM(300, 150, dropout=0.3, bidirectional=True)\n",
       "  (column_encoder): LSTM(300, 150, dropout=0.3, bidirectional=True)\n",
       "  (softmax): Softmax()\n",
       "  (fc1_agg): Linear(in_features=300, out_features=150, bias=True)\n",
       "  (fc2_agg): Linear(in_features=150, out_features=7, bias=True)\n",
       "  (dropout): Dropout(p=0.3)\n",
       "  (attention_1): Linear(in_features=300, out_features=150, bias=True)\n",
       "  (attention_2): Linear(in_features=150, out_features=1, bias=True)\n",
       "  (attention_1_agg): Linear(in_features=300, out_features=150, bias=True)\n",
       "  (attention_2_agg): Linear(in_features=150, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SelectPredictor(nn.Module):\n",
    "    def __init__(self, input_size, lstm_cells, embedding_matrix,\n",
    "                 output_size, train_embedding=False, batch_size=64,\n",
    "                 dropout=0.3, device=device):\n",
    "        super(SelectPredictor, self).__init__()\n",
    "        self.input_size = input_size\n",
    "\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.word_embedding = nn.Embedding(embedding_matrix.shape[0],\n",
    "                                           embedding_matrix.shape[1],\n",
    "                                           _weight=embedding_matrix.clone())\n",
    "        self.word_embedding_agg = nn.Embedding(embedding_matrix.shape[0],\n",
    "                                           embedding_matrix.shape[1],\n",
    "                                           _weight=embedding_matrix.clone())\n",
    "    \n",
    "        self.word_embedding.weight.requires_grad = train_embedding\n",
    "        self.word_embedding_agg.weight.requires_grad = train_embedding\n",
    "\n",
    "        #         self.word_embedding.weight = nn.Parameter(embedding_matrix, requires_grad=True)\n",
    "\n",
    "        self.question_encoder = nn.LSTM(embedding_matrix.shape[1], int(lstm_cells),\n",
    "                                        dropout=dropout, num_layers=1, bidirectional=True)\n",
    "        self.question_encoder_agg = nn.LSTM(embedding_matrix.shape[1], int(lstm_cells),\n",
    "                                        dropout=dropout, num_layers=1, bidirectional=True)\n",
    "\n",
    "        self.column_encoder = nn.LSTM(embedding_matrix.shape[1], int(lstm_cells),\n",
    "                                      dropout=dropout, num_layers=1, bidirectional=True)\n",
    "\n",
    "        self.lstm_cells = lstm_cells\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "        self.fc1_agg = nn.Linear(2 * lstm_cells, lstm_cells)\n",
    "        self.fc2_agg = nn.Linear(lstm_cells, output_size)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        self.attention_1 = nn.Linear(2 * self.lstm_cells, (2 * self.lstm_cells)//2)\n",
    "        self.attention_2 = nn.Linear((2 * self.lstm_cells)//2, 1)\n",
    "        self.attention_1_agg = nn.Linear(2 * self.lstm_cells, (2 * self.lstm_cells)//2)\n",
    "        self.attention_2_agg = nn.Linear((2 * self.lstm_cells)//2, 1)\n",
    "#         self.agg_layer = nn.Linear(2 * )\n",
    "    \n",
    "    def generate_hidden(self, batch_size, n_layers=2):\n",
    "        h_0 = torch.zeros(n_layers, batch_size, self.lstm_cells, requires_grad=False, device=self.device)\n",
    "        c_0 = torch.zeros(n_layers, batch_size, self.lstm_cells, requires_grad=False, device=self.device)\n",
    "        return h_0, c_0\n",
    "    \n",
    "    \n",
    "\n",
    "    def encode_columns(self, columns_sequences):\n",
    "        '''\n",
    "        `columns_sequences` is of shape: (batch_size, columns_per_table, words_per_columns, embedding_hidden_size)\n",
    "        '''\n",
    "        # converting the whole batch into sequence of columns - (batch_size * columns_per_table, words_per_column, embedding_hidden_size)\n",
    "        batch_size, columns_per_table, words_per_column, embedding_hidden_size = columns_sequences.shape\n",
    "        columns_unsequeezed = columns_sequences.view(-1, words_per_column, embedding_hidden_size)\n",
    "\n",
    "        init_states = self.generate_hidden(batch_size * columns_per_table)\n",
    "        columns_encoded, (ht, ct) = self.question_encoder(columns_unsequeezed.permute(1, 0, 2), init_states)\n",
    "\n",
    "        \n",
    "        # column_encoded -> batch_size * columns_per_table, seq_len/#n_words_per_col, hidden_size\n",
    "    \n",
    "        # ht -> #n_layers, batch_size * columns_per_table, hidden_units\n",
    "        \n",
    "        seq_len, _, hidden_size = ht.shape\n",
    "        \n",
    "    \n",
    "        # ht.size() -> (n_layers, batch_size * columns_per_table, hidden_size)\n",
    "#         ht_batched = ht[-1].view(-1, columns_per_table, hidden_size)\n",
    "        \n",
    "    \n",
    "    \n",
    "#         print(ht.shape)\n",
    "#         ht_batched = torch.cat((ht[0], ht[1]), dim=1)\n",
    "        \n",
    "        ht = ht.permute(1, 0, 2).contiguous()\n",
    "#         ht.size() -> (batch_size * columns_per_table, n_layers, hidden_size)\n",
    "        ht_batched = ht.view(batch_size * columns_per_table, -1)\n",
    "        \n",
    "        ht_batched = ht_batched.view(batch_size, columns_per_table, -1)\n",
    "        # ht_batched.shape() -> (batch_size, columns_per_table, lstm_cell * n_layers)\n",
    "        \n",
    "#         c = columns_encoded.permute(1, 0, 2)\n",
    "#         print(ht_batched.shape, 'ht_batched')\n",
    "    \n",
    "    \n",
    "    \n",
    "        # ht_batched.shape() -> (batch_size, columns_per_table, hidden_size)\n",
    "        return ht_batched\n",
    "    \n",
    "    def self_attention(self, output):\n",
    "#         output.shape() -> seq_len, batch_size, n_lstm_cells * n_layers\n",
    "        output = output.permute(1, 0, 2)\n",
    "        atten_weights = self.attention_2(F.tanh(self.attention_1(output))).squeeze(2)\n",
    "#         atten_weights.shape() -> batch_size, seq_len\n",
    "        atten_weights = F.softmax(atten_weights ,dim=1)\n",
    "#         atten_weights.shape() -> batch_size, seq_len\n",
    "#         output.shape() -> batch_size, seq_len, n_lstm_cells * n_layers\n",
    "        attended_output = torch.bmm(output.permute(0, 2, 1), atten_weights.unsqueeze(2)).squeeze(2)\n",
    "#         attented_output.shape() -> batch_size, n_lstm_cells * n_layers\n",
    "#         print(attended_output.shape)\n",
    "\n",
    "        return attended_output\n",
    "   \n",
    "    def self_attention_agg(self, output):\n",
    "#         output.shape() -> seq_len, batch_size, n_lstm_cells * n_layers\n",
    "        output = output.permute(1, 0, 2)\n",
    "        atten_weights = self.attention_2_agg(F.tanh(self.attention_1_agg(output))).squeeze(2)\n",
    "#         atten_weights.shape() -> batch_size, seq_len\n",
    "        atten_weights = F.softmax(atten_weights ,dim=1)\n",
    "#         atten_weights.shape() -> batch_size, seq_len\n",
    "#         output.shape() -> batch_size, seq_len, n_lstm_cells * n_layers\n",
    "        attended_output = torch.bmm(output.permute(0, 2, 1), atten_weights.unsqueeze(2)).squeeze(2)\n",
    "#         attented_output.shape() -> batch_size, n_lstm_cells * n_layers\n",
    "#         print(attended_output.shape)\n",
    "\n",
    "        return attended_output\n",
    "    \n",
    "    def forward_agg(self, questions):\n",
    "        batch_size = questions.shape[0]\n",
    "        embedded_questions = self.dropout(self.word_embedding_agg(questions))\n",
    "        questions_sequence = embedded_questions.permute(1, 0, 2)\n",
    "        init_hidden_states = self.generate_hidden(batch_size)\n",
    "        questions_encoded, (ht, ct) = self.question_encoder_agg(questions_sequence, init_hidden_states)\n",
    "        question_attended = self.self_attention_agg(questions_encoded)\n",
    "        output = self.fc2_agg(torch.tanh(self.fc1_agg(question_attended)))\n",
    "        output_pd = output\n",
    "#         print(output_pd.shape)\n",
    "        return output_pd\n",
    "    \n",
    "    def forward_sel(self, questions, columns):\n",
    "        batch_size = questions.shape[0]\n",
    "        embedded_questions = self.dropout(self.word_embedding(questions))\n",
    "        embedded_columns = self.dropout(self.word_embedding(columns))\n",
    "\n",
    "        # convert to shape -> (seq_len, batch_size, feature_len)\n",
    "        questions_sequence = embedded_questions.permute(1, 0, 2)\n",
    "\n",
    "        init_hidden_states = self.generate_hidden(batch_size)\n",
    "        \n",
    "        \n",
    "        questions_encoded, (ht, ct) = self.question_encoder(questions_sequence, init_hidden_states)\n",
    "        # questions_encoded.size() -> (seq_len, batch_size, hidden_states)\n",
    "        # ht.size() -> (n_layers, batch_size, hidden_states)\n",
    "        \n",
    "        columns_encoded = self.encode_columns(embedded_columns)\n",
    "        # columns_encoded.shape() -> (batch_size, columns_per_table, hidden_states)\n",
    "        \n",
    "        # batch_size_, columns_per_table, hidden_size = columns_encoded.shape\n",
    "        \n",
    "#         question_attended = self.self_attention(questions_encoded, ht)\n",
    "        question_attended = self.self_attention(questions_encoded)\n",
    "#         question_attended = ht[-1]\n",
    "#         attended_hidden.shape -> (batch_size, hidden_size)\n",
    "        \n",
    "#         question_repr =  ht[-1].unsqueeze(2)\n",
    "        question_repr =  question_attended.unsqueeze(2)\n",
    "#         print('question_repr', question_repr.shape, columns_encoded.shape)\n",
    "        # questions_repr.size() -> (batch_size, hidden_states, 1)\n",
    "        \n",
    "        atten_weights = torch.bmm(columns_encoded, question_repr)\n",
    "        # atten_weights.size() -> (batch_size, columns_per_table, 1)\n",
    "        \n",
    "        cols_pd = atten_weights.squeeze(2)\n",
    "        # cols_pd.size() -> (batch_size, columns_per_table)\n",
    "        \n",
    "        return cols_pd\n",
    "\n",
    "    def forward(self, questions, columns):\n",
    "        return self.forward_sel(questions, columns), self.forward_agg(questions)\n",
    "                              \n",
    "\n",
    "            \n",
    "                              \n",
    "\n",
    "\n",
    "sel_nn = SelectPredictor(datatransformer.max_words_per_question, 150,\n",
    "                         embedding_tensors, len(agg_ops), train_embedding=False, dropout=0.3)\n",
    "\n",
    "sel_nn.float()\n",
    "sel_nn.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([0, 1, 2, 3, 4, 5, 6]),\n",
       "  array([40606,  3161,  3231,  5117,  2043,  2201,     4])),\n",
       " ['', 'MAX', 'MIN', 'COUNT', 'SUM', 'AVG', 'COUNT_DISTINCT'])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(train_agg, return_counts=True), agg_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 1, 1: 2, 2: 2, 3: 1, 4: 3, 5: 3, 6: 3}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{0: 1, 1: 2, 2: 2, 3: 1, 4: 3, 5: 3, 6: 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(list(set(train_agg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.98292300e-01, 2.54724997e+00, 2.49206349e+00, 1.57355035e+00,\n",
       "       3.94119292e+00, 3.65827221e+00, 2.01296429e+03])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "agg_weights = class_weight.compute_class_weight('balanced', classes=list(set(train_agg)), y=train_agg)\n",
    "# agg_weights = class_weight.compute_class_weight({0: 1, 1: 2, 2: 2, 3: 1, 4: 3, 5: 3, 6: 3}, classes=np.array(list(set(train_agg))), y=train_agg)\n",
    "# agg_weights = array([,  3161,  3231,  5114,  2042,  2201,     5]))\n",
    "agg_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agg_weights = [1., 2., 2., 3., 6., 4., 4.]\n",
    "# agg_weights = [i/sum(agg_weights) for i in agg_weights]\n",
    "# agg_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval(sony_df.header.values[6]).index('PRIMARY_ACCT_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "# loss_function = nn.CrossEntropyLoss(weight=torch.tensor(agg_weights, device=device, requires_grad=False, dtype=torch.float))\n",
    "#     weight=torch.tensor([1, 2, 2, 2, 2, 2], dtype=torch.float, device=device,\n",
    "#                                                         requires_grad=False))\n",
    "optimizer = optim.Adam(sel_nn.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/project/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:144: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/home/project/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:60: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/home/project/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:126: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loss: 0.6440310920588672, batch: 880"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/project/anaconda3/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type SelectPredictor. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/project/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/project/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'epoch': 0,\n",
      "    'test_accuracy': 0.9093541483066852,\n",
      "    'test_f1': 0.9093771885157609,\n",
      "    'test_loss': 0.41878971457481384,\n",
      "    'train_accuracy': 0.9377960718911342,\n",
      "    'train_f1': 0.9377862100620612,\n",
      "    'train_loss': 0.24262471497058868}\n",
      "[   (   'SELECT COUNT_DISTINCT(GAME_GENRE) FROM t1',\n",
      "        ('Find the Distinct count of games for RPG genre', 'GAME_NAME')),\n",
      "    (   'SELECT COUNT_DISTINCT(GAME_GENRE) FROM t1',\n",
      "        (   'Find the distinct count of games for RPG and Adventure genre',\n",
      "            'GAME_NAME')),\n",
      "    (   'SELECT COUNT_DISTINCT(GLOBAL_TITLE_ID_ID) FROM t1',\n",
      "        (   'Find the number of distinct Title_ID for Horizon Zero Dawn',\n",
      "            'TITLE_ID')),\n",
      "    (   'SELECT COUNT(GAME_NAME) FROM t1',\n",
      "        ('Find the Number of games under franchise 125690', 'GAME_NAME_ID')),\n",
      "    (   'SELECT SUM(PSN_ACCOUNTS) FROM t1',\n",
      "        (   'FIND TOTAL PLAYTIME FOR GAME_NAME_ID = 123456',\n",
      "            'PRIMARY_SESSION_LENGTH_SECONDS')),\n",
      "    (   'SELECT COUNT(PSN_ACCOUNTS) FROM t1',\n",
      "        (   'FIND TOTAL NUMBER OF ACCOUNTS WHO PLAYED GAME_NAME_ID = 123456',\n",
      "            'PRIMARY_account_ID')),\n",
      "    (   'SELECT COUNT(PRIMARY_account_ID) FROM t1',\n",
      "        (   'FIND TOTAL NUMBER OF ACCOUNTS WHO PLAYED ON DATE = 11/20/2018',\n",
      "            'PRIMARY_account_ID')),\n",
      "    (   'SELECT SUM(SPEND_USD_CENTS) FROM t1',\n",
      "        (   'Find total revenue for the title whose title_id=118444 in US.  ',\n",
      "            'SPEND_USD_CENTS')),\n",
      "    (   'SELECT COUNT_DISTINCT(account_ID) FROM t1',\n",
      "        ('Find number of accounts in region 4 ', 'account_ID')),\n",
      "    (   'SELECT SUM(TRANSACTION_ID) FROM t1',\n",
      "        ('Find total sales for the title whose title_id=118444', 'QUANTITY')),\n",
      "    (   'SELECT COUNT(TRANSACTION_ID) FROM t1',\n",
      "        (   'Count number of accounts of who made transaction on 11/26/2018.',\n",
      "            'account_ID')),\n",
      "    (   'SELECT COUNT_DISTINCT(TROPHY_ID) FROM t1',\n",
      "        (   'How many distinct trophy ids are there for trophy name \"Battle '\n",
      "            'for Control\"',\n",
      "            'TROPHY_ID')),\n",
      "    (   'SELECT (TROPHY_PID) FROM t1',\n",
      "        (   \"List of trophies that are marked under title '13333'\",\n",
      "            'TROPHY_NAME')),\n",
      "    (   'SELECT (TROPHY_TYPE) FROM t1',\n",
      "        (   'What is the trophy_type of trophy name \"The Ballerina\"',\n",
      "            'TROPHY_TYPE')),\n",
      "    (   'SELECT COUNT(FRANCHISE_NAME_ID) FROM t1',\n",
      "        (   'How many franchisee names are present for franchisee name id '\n",
      "            '75230',\n",
      "            'FRANCHISE_NAME')),\n",
      "    (   'SELECT COUNT(FRANCHISE_NAME) FROM t1',\n",
      "        (   'How many Game names are present for franchisee name id 75230',\n",
      "            'GAME_NAME')),\n",
      "    (   'SELECT COUNT(FRANCHISE_NAME_ID) FROM t1',\n",
      "        (   'How many franchisee names are present for Party Type 3rd Party',\n",
      "            'FRANCHISE_NAME')),\n",
      "    (   'SELECT (GAME_GENRE) FROM t1',\n",
      "        (   'What is/are the game genre(s) for franchisee name id 75230',\n",
      "            'GAME_GENRE'))]\n",
      " Loss: 0.26410805051722985, batch: 880\n",
      "\n",
      "{   'epoch': 1,\n",
      "    'test_accuracy': 0.9209996223089513,\n",
      "    'test_f1': 0.9209935325086306,\n",
      "    'test_loss': 0.3633750379085541,\n",
      "    'train_accuracy': 0.9619963451200255,\n",
      "    'train_f1': 0.9619798772925907,\n",
      "    'train_loss': 0.14614234864711761}\n",
      "[   (   'SELECT COUNT_DISTINCT(GAME_GENRE) FROM t1',\n",
      "        ('Find the Distinct count of games for RPG genre', 'GAME_NAME')),\n",
      "    (   'SELECT COUNT_DISTINCT(GAME_GENRE) FROM t1',\n",
      "        (   'Find the distinct count of games for RPG and Adventure genre',\n",
      "            'GAME_NAME')),\n",
      "    (   'SELECT COUNT_DISTINCT(FRANCHISE_NAME_ID) FROM t1',\n",
      "        (   'Find the number of distinct Title_ID for Horizon Zero Dawn',\n",
      "            'TITLE_ID')),\n",
      "    (   'SELECT COUNT(GAME_NAME) FROM t1',\n",
      "        ('Find the Number of games under franchise 125690', 'GAME_NAME_ID')),\n",
      "    (   'SELECT SUM(SECONDARY_GAMEPLAY_IND) FROM t1',\n",
      "        (   'FIND TOTAL PLAYTIME FOR GAME_NAME_ID = 123456',\n",
      "            'PRIMARY_SESSION_LENGTH_SECONDS')),\n",
      "    (   'SELECT COUNT(PRIMARY_account_ID) FROM t1',\n",
      "        (   'FIND TOTAL NUMBER OF ACCOUNTS WHO PLAYED GAME_NAME_ID = 123456',\n",
      "            'PRIMARY_account_ID')),\n",
      "    (   'SELECT COUNT(PRIMARY_account_ID) FROM t1',\n",
      "        (   'FIND TOTAL NUMBER OF ACCOUNTS WHO PLAYED ON DATE = 11/20/2018',\n",
      "            'PRIMARY_account_ID')),\n",
      "    (   'SELECT SUM(SPEND_USD_CENTS) FROM t1',\n",
      "        (   'Find total revenue for the title whose title_id=118444 in US.  ',\n",
      "            'SPEND_USD_CENTS')),\n",
      "    (   'SELECT COUNT_DISTINCT(account_ID) FROM t1',\n",
      "        ('Find number of accounts in region 4 ', 'account_ID')),\n",
      "    (   'SELECT SUM(IMMEDIACY_DAYS) FROM t1',\n",
      "        ('Find total sales for the title whose title_id=118444', 'QUANTITY')),\n",
      "    (   'SELECT COUNT(account_ID) FROM t1',\n",
      "        (   'Count number of accounts of who made transaction on 11/26/2018.',\n",
      "            'account_ID')),\n",
      "    (   'SELECT COUNT_DISTINCT(TROPHY_ID) FROM t1',\n",
      "        (   'How many distinct trophy ids are there for trophy name \"Battle '\n",
      "            'for Control\"',\n",
      "            'TROPHY_ID')),\n",
      "    (   'SELECT (TROPHY_PID) FROM t1',\n",
      "        (   \"List of trophies that are marked under title '13333'\",\n",
      "            'TROPHY_NAME')),\n",
      "    (   'SELECT (TROPHY_PID) FROM t1',\n",
      "        (   'What is the trophy_type of trophy name \"The Ballerina\"',\n",
      "            'TROPHY_TYPE')),\n",
      "    (   'SELECT COUNT(FRANCHISE_NAME) FROM t1',\n",
      "        (   'How many franchisee names are present for franchisee name id '\n",
      "            '75230',\n",
      "            'FRANCHISE_NAME')),\n",
      "    (   'SELECT COUNT(GAME_NAME) FROM t1',\n",
      "        (   'How many Game names are present for franchisee name id 75230',\n",
      "            'GAME_NAME')),\n",
      "    (   'SELECT COUNT(FRANCHISE_NAME) FROM t1',\n",
      "        (   'How many franchisee names are present for Party Type 3rd Party',\n",
      "            'FRANCHISE_NAME')),\n",
      "    (   'SELECT (GAME_GENRE) FROM t1',\n",
      "        (   'What is/are the game genre(s) for franchisee name id 75230',\n",
      "            'GAME_GENRE'))]\n",
      " Loss: 0.17632719837552444, batch: 880\n",
      "\n",
      "{   'epoch': 2,\n",
      "    'test_accuracy': 0.9251542238448949,\n",
      "    'test_f1': 0.9251401802470329,\n",
      "    'test_loss': 0.3785722553730011,\n",
      "    'train_accuracy': 0.9727658215495981,\n",
      "    'train_f1': 0.972746469712058,\n",
      "    'train_loss': 0.10262884944677353}\n",
      "[   (   'SELECT COUNT_DISTINCT(GAME_NAME) FROM t1',\n",
      "        ('Find the Distinct count of games for RPG genre', 'GAME_NAME')),\n",
      "    (   'SELECT COUNT_DISTINCT(GAME_NAME) FROM t1',\n",
      "        (   'Find the distinct count of games for RPG and Adventure genre',\n",
      "            'GAME_NAME')),\n",
      "    (   'SELECT COUNT_DISTINCT(TITLE_ID) FROM t1',\n",
      "        (   'Find the number of distinct Title_ID for Horizon Zero Dawn',\n",
      "            'TITLE_ID')),\n",
      "    (   'SELECT COUNT(GAME_NAME) FROM t1',\n",
      "        ('Find the Number of games under franchise 125690', 'GAME_NAME_ID')),\n",
      "    (   'SELECT SUM(SECONDARY_GAMEPLAY_IND) FROM t1',\n",
      "        (   'FIND TOTAL PLAYTIME FOR GAME_NAME_ID = 123456',\n",
      "            'PRIMARY_SESSION_LENGTH_SECONDS')),\n",
      "    (   'SELECT COUNT(LOCAL_ACCOUNTS) FROM t1',\n",
      "        (   'FIND TOTAL NUMBER OF ACCOUNTS WHO PLAYED GAME_NAME_ID = 123456',\n",
      "            'PRIMARY_account_ID')),\n",
      "    (   'SELECT COUNT(LOCAL_ACCOUNTS) FROM t1',\n",
      "        (   'FIND TOTAL NUMBER OF ACCOUNTS WHO PLAYED ON DATE = 11/20/2018',\n",
      "            'PRIMARY_account_ID')),\n",
      "    (   'SELECT SUM(SPEND_USD_CENTS) FROM t1',\n",
      "        (   'Find total revenue for the title whose title_id=118444 in US.  ',\n",
      "            'SPEND_USD_CENTS')),\n",
      "    (   'SELECT COUNT_DISTINCT(account_ID) FROM t1',\n",
      "        ('Find number of accounts in region 4 ', 'account_ID')),\n",
      "    (   'SELECT SUM(SPEND_USD_CENTS) FROM t1',\n",
      "        ('Find total sales for the title whose title_id=118444', 'QUANTITY')),\n",
      "    (   'SELECT COUNT(account_ID) FROM t1',\n",
      "        (   'Count number of accounts of who made transaction on 11/26/2018.',\n",
      "            'account_ID')),\n",
      "    (   'SELECT COUNT_DISTINCT(TROPHY_ID) FROM t1',\n",
      "        (   'How many distinct trophy ids are there for trophy name \"Battle '\n",
      "            'for Control\"',\n",
      "            'TROPHY_ID')),\n",
      "    (   'SELECT (TROPHY_PID) FROM t1',\n",
      "        (   \"List of trophies that are marked under title '13333'\",\n",
      "            'TROPHY_NAME')),\n",
      "    (   'SELECT (TROPHY_PID) FROM t1',\n",
      "        (   'What is the trophy_type of trophy name \"The Ballerina\"',\n",
      "            'TROPHY_TYPE')),\n",
      "    (   'SELECT COUNT(FRANCHISE_NAME) FROM t1',\n",
      "        (   'How many franchisee names are present for franchisee name id '\n",
      "            '75230',\n",
      "            'FRANCHISE_NAME')),\n",
      "    (   'SELECT COUNT(GAME_NAME) FROM t1',\n",
      "        (   'How many Game names are present for franchisee name id 75230',\n",
      "            'GAME_NAME')),\n",
      "    (   'SELECT COUNT(FRANCHISE_NAME) FROM t1',\n",
      "        (   'How many franchisee names are present for Party Type 3rd Party',\n",
      "            'FRANCHISE_NAME')),\n",
      "    (   'SELECT (GAME_GENRE) FROM t1',\n",
      "        (   'What is/are the game genre(s) for franchisee name id 75230',\n",
      "            'GAME_GENRE'))]\n",
      " Loss: 0.13367196626283906, batch: 880\n",
      "\n",
      "{   'epoch': 3,\n",
      "    'test_accuracy': 0.9276092156615888,\n",
      "    'test_f1': 0.9275791669258833,\n",
      "    'test_loss': 0.374660462141037,\n",
      "    'train_accuracy': 0.9790997640295939,\n",
      "    'train_f1': 0.9790964516630091,\n",
      "    'train_loss': 0.07926459610462189}\n",
      "[   (   'SELECT COUNT_DISTINCT(GAME_NAME) FROM t1',\n",
      "        ('Find the Distinct count of games for RPG genre', 'GAME_NAME')),\n",
      "    (   'SELECT COUNT_DISTINCT(GAME_NAME) FROM t1',\n",
      "        (   'Find the distinct count of games for RPG and Adventure genre',\n",
      "            'GAME_NAME')),\n",
      "    (   'SELECT COUNT_DISTINCT(TITLE_ID) FROM t1',\n",
      "        (   'Find the number of distinct Title_ID for Horizon Zero Dawn',\n",
      "            'TITLE_ID')),\n",
      "    (   'SELECT COUNT(GAME_NAME) FROM t1',\n",
      "        ('Find the Number of games under franchise 125690', 'GAME_NAME_ID')),\n",
      "    (   'SELECT SUM(SECONDARY_GAMEPLAY_IND) FROM t1',\n",
      "        (   'FIND TOTAL PLAYTIME FOR GAME_NAME_ID = 123456',\n",
      "            'PRIMARY_SESSION_LENGTH_SECONDS')),\n",
      "    (   'SELECT COUNT(PRIMARY_account_ID) FROM t1',\n",
      "        (   'FIND TOTAL NUMBER OF ACCOUNTS WHO PLAYED GAME_NAME_ID = 123456',\n",
      "            'PRIMARY_account_ID')),\n",
      "    (   'SELECT COUNT(PRIMARY_account_ID) FROM t1',\n",
      "        (   'FIND TOTAL NUMBER OF ACCOUNTS WHO PLAYED ON DATE = 11/20/2018',\n",
      "            'PRIMARY_account_ID')),\n",
      "    (   'SELECT SUM(SPEND_USD_CENTS) FROM t1',\n",
      "        (   'Find total revenue for the title whose title_id=118444 in US.  ',\n",
      "            'SPEND_USD_CENTS')),\n",
      "    (   'SELECT COUNT_DISTINCT(account_ID) FROM t1',\n",
      "        ('Find number of accounts in region 4 ', 'account_ID')),\n",
      "    (   'SELECT SUM(IMMEDIACY_DAYS) FROM t1',\n",
      "        ('Find total sales for the title whose title_id=118444', 'QUANTITY')),\n",
      "    (   'SELECT COUNT(account_ID) FROM t1',\n",
      "        (   'Count number of accounts of who made transaction on 11/26/2018.',\n",
      "            'account_ID')),\n",
      "    (   'SELECT COUNT_DISTINCT(TROPHY_ID) FROM t1',\n",
      "        (   'How many distinct trophy ids are there for trophy name \"Battle '\n",
      "            'for Control\"',\n",
      "            'TROPHY_ID')),\n",
      "    (   'SELECT (TROPHY_PID) FROM t1',\n",
      "        (   \"List of trophies that are marked under title '13333'\",\n",
      "            'TROPHY_NAME')),\n",
      "    (   'SELECT (TROPHY_ID) FROM t1',\n",
      "        (   'What is the trophy_type of trophy name \"The Ballerina\"',\n",
      "            'TROPHY_TYPE')),\n",
      "    (   'SELECT COUNT(FRANCHISE_NAME) FROM t1',\n",
      "        (   'How many franchisee names are present for franchisee name id '\n",
      "            '75230',\n",
      "            'FRANCHISE_NAME')),\n",
      "    (   'SELECT COUNT(GAME_NAME) FROM t1',\n",
      "        (   'How many Game names are present for franchisee name id 75230',\n",
      "            'GAME_NAME')),\n",
      "    (   'SELECT COUNT(FRANCHISE_NAME) FROM t1',\n",
      "        (   'How many franchisee names are present for Party Type 3rd Party',\n",
      "            'FRANCHISE_NAME')),\n",
      "    (   'SELECT (GAME_GENRE) FROM t1',\n",
      "        (   'What is/are the game genre(s) for franchisee name id 75230',\n",
      "            'GAME_GENRE'))]\n",
      " Loss: 0.10529459500896998, batch: 880\n",
      "\n",
      "{   'epoch': 4,\n",
      "    'test_accuracy': 0.929938310462042,\n",
      "    'test_f1': 0.9299316314621849,\n",
      "    'test_loss': 0.4007880389690399,\n",
      "    'train_accuracy': 0.9833223923495911,\n",
      "    'train_f1': 0.9833314365232232,\n",
      "    'train_loss': 0.06489952653646469}\n",
      "[   (   'SELECT COUNT_DISTINCT(GAME_NAME) FROM t1',\n",
      "        ('Find the Distinct count of games for RPG genre', 'GAME_NAME')),\n",
      "    (   'SELECT COUNT_DISTINCT(GAME_NAME) FROM t1',\n",
      "        (   'Find the distinct count of games for RPG and Adventure genre',\n",
      "            'GAME_NAME')),\n",
      "    (   'SELECT COUNT_DISTINCT(TITLE_ID) FROM t1',\n",
      "        (   'Find the number of distinct Title_ID for Horizon Zero Dawn',\n",
      "            'TITLE_ID')),\n",
      "    (   'SELECT COUNT(GAME_NAME) FROM t1',\n",
      "        ('Find the Number of games under franchise 125690', 'GAME_NAME_ID')),\n",
      "    (   'SELECT SUM(PRIMARY_SESSION_LENGTH_SECONDS) FROM t1',\n",
      "        (   'FIND TOTAL PLAYTIME FOR GAME_NAME_ID = 123456',\n",
      "            'PRIMARY_SESSION_LENGTH_SECONDS')),\n",
      "    (   'SELECT COUNT(PSN_ACCOUNTS) FROM t1',\n",
      "        (   'FIND TOTAL NUMBER OF ACCOUNTS WHO PLAYED GAME_NAME_ID = 123456',\n",
      "            'PRIMARY_account_ID')),\n",
      "    (   'SELECT COUNT(PRIMARY_account_ID) FROM t1',\n",
      "        (   'FIND TOTAL NUMBER OF ACCOUNTS WHO PLAYED ON DATE = 11/20/2018',\n",
      "            'PRIMARY_account_ID')),\n",
      "    (   'SELECT SUM(PSN_TENURE_DAYS) FROM t1',\n",
      "        (   'Find total revenue for the title whose title_id=118444 in US.  ',\n",
      "            'SPEND_USD_CENTS')),\n",
      "    (   'SELECT COUNT_DISTINCT(account_ID) FROM t1',\n",
      "        ('Find number of accounts in region 4 ', 'account_ID')),\n",
      "    (   'SELECT SUM(IMMEDIACY_DAYS) FROM t1',\n",
      "        ('Find total sales for the title whose title_id=118444', 'QUANTITY')),\n",
      "    (   'SELECT COUNT(TRANSACTION_ID) FROM t1',\n",
      "        (   'Count number of accounts of who made transaction on 11/26/2018.',\n",
      "            'account_ID')),\n",
      "    (   'SELECT COUNT_DISTINCT(TROPHY_ID) FROM t1',\n",
      "        (   'How many distinct trophy ids are there for trophy name \"Battle '\n",
      "            'for Control\"',\n",
      "            'TROPHY_ID')),\n",
      "    (   'SELECT (TROPHY_PID) FROM t1',\n",
      "        (   \"List of trophies that are marked under title '13333'\",\n",
      "            'TROPHY_NAME')),\n",
      "    (   'SELECT (TROPHY_TYPE) FROM t1',\n",
      "        (   'What is the trophy_type of trophy name \"The Ballerina\"',\n",
      "            'TROPHY_TYPE')),\n",
      "    (   'SELECT COUNT(FRANCHISE_NAME) FROM t1',\n",
      "        (   'How many franchisee names are present for franchisee name id '\n",
      "            '75230',\n",
      "            'FRANCHISE_NAME')),\n",
      "    (   'SELECT COUNT(GAME_NAME) FROM t1',\n",
      "        (   'How many Game names are present for franchisee name id 75230',\n",
      "            'GAME_NAME')),\n",
      "    (   'SELECT COUNT(FRANCHISE_NAME) FROM t1',\n",
      "        (   'How many franchisee names are present for Party Type 3rd Party',\n",
      "            'FRANCHISE_NAME')),\n",
      "    (   'SELECT (GAME_GENRE) FROM t1',\n",
      "        (   'What is/are the game genre(s) for franchisee name id 75230',\n",
      "            'GAME_GENRE'))]\n",
      " Loss: 0.08387282948559997, batch: 880\n",
      "\n",
      "{   'epoch': 5,\n",
      "    'test_accuracy': 0.9306936925594863,\n",
      "    'test_f1': 0.930748868177582,\n",
      "    'test_loss': 0.4019424319267273,\n",
      "    'train_accuracy': 0.9864804925216898,\n",
      "    'train_f1': 0.9864852909766841,\n",
      "    'train_loss': 0.05083535611629486}\n",
      "[   (   'SELECT COUNT_DISTINCT(GAME_NAME) FROM t1',\n",
      "        ('Find the Distinct count of games for RPG genre', 'GAME_NAME')),\n",
      "    (   'SELECT COUNT_DISTINCT(GAME_NAME) FROM t1',\n",
      "        (   'Find the distinct count of games for RPG and Adventure genre',\n",
      "            'GAME_NAME')),\n",
      "    (   'SELECT COUNT_DISTINCT(TITLE_ID) FROM t1',\n",
      "        (   'Find the number of distinct Title_ID for Horizon Zero Dawn',\n",
      "            'TITLE_ID')),\n",
      "    (   'SELECT COUNT(GAME_NAME) FROM t1',\n",
      "        ('Find the Number of games under franchise 125690', 'GAME_NAME_ID')),\n",
      "    (   'SELECT SUM(PRIMARY_SESSION_LENGTH_SECONDS) FROM t1',\n",
      "        (   'FIND TOTAL PLAYTIME FOR GAME_NAME_ID = 123456',\n",
      "            'PRIMARY_SESSION_LENGTH_SECONDS')),\n",
      "    (   'SELECT COUNT(PRIMARY_account_ID) FROM t1',\n",
      "        (   'FIND TOTAL NUMBER OF ACCOUNTS WHO PLAYED GAME_NAME_ID = 123456',\n",
      "            'PRIMARY_account_ID')),\n",
      "    (   'SELECT COUNT(PRIMARY_account_ID) FROM t1',\n",
      "        (   'FIND TOTAL NUMBER OF ACCOUNTS WHO PLAYED ON DATE = 11/20/2018',\n",
      "            'PRIMARY_account_ID')),\n",
      "    (   'SELECT SUM(SPEND_USD_CENTS) FROM t1',\n",
      "        (   'Find total revenue for the title whose title_id=118444 in US.  ',\n",
      "            'SPEND_USD_CENTS')),\n",
      "    (   'SELECT COUNT_DISTINCT(account_ID) FROM t1',\n",
      "        ('Find number of accounts in region 4 ', 'account_ID')),\n",
      "    (   'SELECT SUM(SPEND_USD_CENTS) FROM t1',\n",
      "        ('Find total sales for the title whose title_id=118444', 'QUANTITY')),\n",
      "    (   'SELECT COUNT(account_ID) FROM t1',\n",
      "        (   'Count number of accounts of who made transaction on 11/26/2018.',\n",
      "            'account_ID')),\n",
      "    (   'SELECT COUNT_DISTINCT(TROPHY_ID) FROM t1',\n",
      "        (   'How many distinct trophy ids are there for trophy name \"Battle '\n",
      "            'for Control\"',\n",
      "            'TROPHY_ID')),\n",
      "    (   'SELECT (TROPHY_PID) FROM t1',\n",
      "        (   \"List of trophies that are marked under title '13333'\",\n",
      "            'TROPHY_NAME')),\n",
      "    (   'SELECT (TROPHY_TYPE) FROM t1',\n",
      "        (   'What is the trophy_type of trophy name \"The Ballerina\"',\n",
      "            'TROPHY_TYPE')),\n",
      "    (   'SELECT COUNT(FRANCHISE_NAME) FROM t1',\n",
      "        (   'How many franchisee names are present for franchisee name id '\n",
      "            '75230',\n",
      "            'FRANCHISE_NAME')),\n",
      "    (   'SELECT COUNT(GAME_NAME) FROM t1',\n",
      "        (   'How many Game names are present for franchisee name id 75230',\n",
      "            'GAME_NAME')),\n",
      "    (   'SELECT COUNT(FRANCHISE_NAME) FROM t1',\n",
      "        (   'How many franchisee names are present for Party Type 3rd Party',\n",
      "            'FRANCHISE_NAME')),\n",
      "    (   'SELECT (GAME_GENRE) FROM t1',\n",
      "        (   'What is/are the game genre(s) for franchisee name id 75230',\n",
      "            'GAME_GENRE'))]\n",
      " Loss: 0.07084443177816203, batch: 880\n",
      "\n",
      "{   'epoch': 6,\n",
      "    'test_accuracy': 0.9314490746569306,\n",
      "    'test_f1': 0.9314976839004344,\n",
      "    'test_loss': 0.3998188078403473,\n",
      "    'train_accuracy': 0.9887160016322765,\n",
      "    'train_f1': 0.988717010029807,\n",
      "    'train_loss': 0.041363269090652466}\n",
      "[   (   'SELECT COUNT_DISTINCT(GAME_NAME) FROM t1',\n",
      "        ('Find the Distinct count of games for RPG genre', 'GAME_NAME')),\n",
      "    (   'SELECT COUNT_DISTINCT(GAME_NAME) FROM t1',\n",
      "        (   'Find the distinct count of games for RPG and Adventure genre',\n",
      "            'GAME_NAME')),\n",
      "    (   'SELECT COUNT_DISTINCT(TITLE_ID) FROM t1',\n",
      "        (   'Find the number of distinct Title_ID for Horizon Zero Dawn',\n",
      "            'TITLE_ID')),\n",
      "    (   'SELECT COUNT(GAME_NAME) FROM t1',\n",
      "        ('Find the Number of games under franchise 125690', 'GAME_NAME_ID')),\n",
      "    (   'SELECT SUM(PRIMARY_SESSION_LENGTH_SECONDS) FROM t1',\n",
      "        (   'FIND TOTAL PLAYTIME FOR GAME_NAME_ID = 123456',\n",
      "            'PRIMARY_SESSION_LENGTH_SECONDS')),\n",
      "    (   'SELECT COUNT(PRIMARY_account_ID) FROM t1',\n",
      "        (   'FIND TOTAL NUMBER OF ACCOUNTS WHO PLAYED GAME_NAME_ID = 123456',\n",
      "            'PRIMARY_account_ID')),\n",
      "    (   'SELECT COUNT(PRIMARY_account_ID) FROM t1',\n",
      "        (   'FIND TOTAL NUMBER OF ACCOUNTS WHO PLAYED ON DATE = 11/20/2018',\n",
      "            'PRIMARY_account_ID')),\n",
      "    (   'SELECT SUM(SPEND_USD_CENTS) FROM t1',\n",
      "        (   'Find total revenue for the title whose title_id=118444 in US.  ',\n",
      "            'SPEND_USD_CENTS')),\n",
      "    (   'SELECT COUNT_DISTINCT(account_ID) FROM t1',\n",
      "        ('Find number of accounts in region 4 ', 'account_ID')),\n",
      "    (   'SELECT SUM(SPEND_USD_CENTS) FROM t1',\n",
      "        ('Find total sales for the title whose title_id=118444', 'QUANTITY')),\n",
      "    (   'SELECT COUNT(account_ID) FROM t1',\n",
      "        (   'Count number of accounts of who made transaction on 11/26/2018.',\n",
      "            'account_ID')),\n",
      "    (   'SELECT COUNT_DISTINCT(TROPHY_ID) FROM t1',\n",
      "        (   'How many distinct trophy ids are there for trophy name \"Battle '\n",
      "            'for Control\"',\n",
      "            'TROPHY_ID')),\n",
      "    (   'SELECT (TROPHY_PID) FROM t1',\n",
      "        (   \"List of trophies that are marked under title '13333'\",\n",
      "            'TROPHY_NAME')),\n",
      "    (   'SELECT (TROPHY_TYPE_ID) FROM t1',\n",
      "        (   'What is the trophy_type of trophy name \"The Ballerina\"',\n",
      "            'TROPHY_TYPE')),\n",
      "    (   'SELECT COUNT(FRANCHISE_NAME) FROM t1',\n",
      "        (   'How many franchisee names are present for franchisee name id '\n",
      "            '75230',\n",
      "            'FRANCHISE_NAME')),\n",
      "    (   'SELECT COUNT(GAME_NAME) FROM t1',\n",
      "        (   'How many Game names are present for franchisee name id 75230',\n",
      "            'GAME_NAME')),\n",
      "    (   'SELECT COUNT(FRANCHISE_NAME) FROM t1',\n",
      "        (   'How many franchisee names are present for Party Type 3rd Party',\n",
      "            'FRANCHISE_NAME')),\n",
      "    (   'SELECT (GAME_GENRE_ID) FROM t1',\n",
      "        (   'What is/are the game genre(s) for franchisee name id 75230',\n",
      "            'GAME_GENRE'))]\n",
      " Loss: 0.057086476820140064, batch: 880\n",
      "\n",
      "{   'epoch': 7,\n",
      "    'test_accuracy': 0.9311343321163288,\n",
      "    'test_f1': 0.9312285590887105,\n",
      "    'test_loss': 0.4166741371154785,\n",
      "    'train_accuracy': 0.9903482781257208,\n",
      "    'train_f1': 0.9903622149415381,\n",
      "    'train_loss': 0.03409074991941452}\n",
      "[   (   'SELECT COUNT_DISTINCT(GAME_NAME) FROM t1',\n",
      "        ('Find the Distinct count of games for RPG genre', 'GAME_NAME')),\n",
      "    (   'SELECT COUNT_DISTINCT(GAME_NAME) FROM t1',\n",
      "        (   'Find the distinct count of games for RPG and Adventure genre',\n",
      "            'GAME_NAME')),\n",
      "    (   'SELECT COUNT_DISTINCT(TITLE_ID) FROM t1',\n",
      "        (   'Find the number of distinct Title_ID for Horizon Zero Dawn',\n",
      "            'TITLE_ID')),\n",
      "    (   'SELECT COUNT(GAME_NAME) FROM t1',\n",
      "        ('Find the Number of games under franchise 125690', 'GAME_NAME_ID')),\n",
      "    (   'SELECT SUM(PRIMARY_SESSION_LENGTH_SECONDS) FROM t1',\n",
      "        (   'FIND TOTAL PLAYTIME FOR GAME_NAME_ID = 123456',\n",
      "            'PRIMARY_SESSION_LENGTH_SECONDS')),\n",
      "    (   'SELECT COUNT(PRIMARY_account_ID) FROM t1',\n",
      "        (   'FIND TOTAL NUMBER OF ACCOUNTS WHO PLAYED GAME_NAME_ID = 123456',\n",
      "            'PRIMARY_account_ID')),\n",
      "    (   'SELECT COUNT(PRIMARY_account_ID) FROM t1',\n",
      "        (   'FIND TOTAL NUMBER OF ACCOUNTS WHO PLAYED ON DATE = 11/20/2018',\n",
      "            'PRIMARY_account_ID')),\n",
      "    (   'SELECT SUM(SPEND_USD_CENTS) FROM t1',\n",
      "        (   'Find total revenue for the title whose title_id=118444 in US.  ',\n",
      "            'SPEND_USD_CENTS')),\n",
      "    (   'SELECT COUNT_DISTINCT(account_ID) FROM t1',\n",
      "        ('Find number of accounts in region 4 ', 'account_ID')),\n",
      "    (   'SELECT SUM(PSN_TENURE_DAYS) FROM t1',\n",
      "        ('Find total sales for the title whose title_id=118444', 'QUANTITY')),\n",
      "    (   'SELECT COUNT(account_ID) FROM t1',\n",
      "        (   'Count number of accounts of who made transaction on 11/26/2018.',\n",
      "            'account_ID')),\n",
      "    (   'SELECT COUNT_DISTINCT(TROPHY_ID) FROM t1',\n",
      "        (   'How many distinct trophy ids are there for trophy name \"Battle '\n",
      "            'for Control\"',\n",
      "            'TROPHY_ID')),\n",
      "    (   'SELECT (TROPHY_PID) FROM t1',\n",
      "        (   \"List of trophies that are marked under title '13333'\",\n",
      "            'TROPHY_NAME')),\n",
      "    (   'SELECT (TROPHY_TYPE) FROM t1',\n",
      "        (   'What is the trophy_type of trophy name \"The Ballerina\"',\n",
      "            'TROPHY_TYPE')),\n",
      "    (   'SELECT COUNT(FRANCHISE_NAME) FROM t1',\n",
      "        (   'How many franchisee names are present for franchisee name id '\n",
      "            '75230',\n",
      "            'FRANCHISE_NAME')),\n",
      "    (   'SELECT COUNT(GAME_NAME) FROM t1',\n",
      "        (   'How many Game names are present for franchisee name id 75230',\n",
      "            'GAME_NAME')),\n",
      "    (   'SELECT COUNT(FRANCHISE_NAME) FROM t1',\n",
      "        (   'How many franchisee names are present for Party Type 3rd Party',\n",
      "            'FRANCHISE_NAME')),\n",
      "    (   'SELECT (GAME_GENRE_ID) FROM t1',\n",
      "        (   'What is/are the game genre(s) for franchisee name id 75230',\n",
      "            'GAME_GENRE'))]\n",
      " Loss: 0.051269048894755545, batch: 880\n",
      "\n",
      "{   'epoch': 8,\n",
      "    'test_accuracy': 0.9310713836082085,\n",
      "    'test_f1': 0.931075453625182,\n",
      "    'test_loss': 0.44233444333076477,\n",
      "    'train_accuracy': 0.9913773220020226,\n",
      "    'train_f1': 0.9913790566087948,\n",
      "    'train_loss': 0.03089013509452343}\n",
      "[   (   'SELECT COUNT_DISTINCT(GAME_NAME) FROM t1',\n",
      "        ('Find the Distinct count of games for RPG genre', 'GAME_NAME')),\n",
      "    (   'SELECT COUNT_DISTINCT(GAME_NAME) FROM t1',\n",
      "        (   'Find the distinct count of games for RPG and Adventure genre',\n",
      "            'GAME_NAME')),\n",
      "    (   'SELECT COUNT_DISTINCT(TITLE_ID) FROM t1',\n",
      "        (   'Find the number of distinct Title_ID for Horizon Zero Dawn',\n",
      "            'TITLE_ID')),\n",
      "    (   'SELECT COUNT(GAME_NAME) FROM t1',\n",
      "        ('Find the Number of games under franchise 125690', 'GAME_NAME_ID')),\n",
      "    (   'SELECT SUM(PRIMARY_SESSION_LENGTH_SECONDS) FROM t1',\n",
      "        (   'FIND TOTAL PLAYTIME FOR GAME_NAME_ID = 123456',\n",
      "            'PRIMARY_SESSION_LENGTH_SECONDS')),\n",
      "    (   'SELECT COUNT(PRIMARY_account_ID) FROM t1',\n",
      "        (   'FIND TOTAL NUMBER OF ACCOUNTS WHO PLAYED GAME_NAME_ID = 123456',\n",
      "            'PRIMARY_account_ID')),\n",
      "    (   'SELECT COUNT(PRIMARY_account_ID) FROM t1',\n",
      "        (   'FIND TOTAL NUMBER OF ACCOUNTS WHO PLAYED ON DATE = 11/20/2018',\n",
      "            'PRIMARY_account_ID')),\n",
      "    (   'SELECT SUM(PSN_TENURE_DAYS) FROM t1',\n",
      "        (   'Find total revenue for the title whose title_id=118444 in US.  ',\n",
      "            'SPEND_USD_CENTS')),\n",
      "    (   'SELECT COUNT_DISTINCT(account_ID) FROM t1',\n",
      "        ('Find number of accounts in region 4 ', 'account_ID')),\n",
      "    (   'SELECT SUM(TIMESTAMP) FROM t1',\n",
      "        ('Find total sales for the title whose title_id=118444', 'QUANTITY')),\n",
      "    (   'SELECT COUNT(account_ID) FROM t1',\n",
      "        (   'Count number of accounts of who made transaction on 11/26/2018.',\n",
      "            'account_ID')),\n",
      "    (   'SELECT COUNT_DISTINCT(TROPHY_ID) FROM t1',\n",
      "        (   'How many distinct trophy ids are there for trophy name \"Battle '\n",
      "            'for Control\"',\n",
      "            'TROPHY_ID')),\n",
      "    (   'SELECT (TROPHY_PID) FROM t1',\n",
      "        (   \"List of trophies that are marked under title '13333'\",\n",
      "            'TROPHY_NAME')),\n",
      "    (   'SELECT (TROPHY_TYPE) FROM t1',\n",
      "        (   'What is the trophy_type of trophy name \"The Ballerina\"',\n",
      "            'TROPHY_TYPE')),\n",
      "    (   'SELECT COUNT(FRANCHISE_NAME) FROM t1',\n",
      "        (   'How many franchisee names are present for franchisee name id '\n",
      "            '75230',\n",
      "            'FRANCHISE_NAME')),\n",
      "    (   'SELECT COUNT(GAME_NAME) FROM t1',\n",
      "        (   'How many Game names are present for franchisee name id 75230',\n",
      "            'GAME_NAME')),\n",
      "    (   'SELECT COUNT(FRANCHISE_NAME) FROM t1',\n",
      "        (   'How many franchisee names are present for Party Type 3rd Party',\n",
      "            'FRANCHISE_NAME')),\n",
      "    (   'SELECT (GAME_GENRE) FROM t1',\n",
      "        (   'What is/are the game genre(s) for franchisee name id 75230',\n",
      "            'GAME_GENRE'))]\n",
      " Loss: 0.04327925425446169, batch: 880\n",
      "\n",
      "{   'epoch': 9,\n",
      "    'test_accuracy': 0.9311343321163288,\n",
      "    'test_f1': 0.9312049598425338,\n",
      "    'test_loss': 0.45047008991241455,\n",
      "    'train_accuracy': 0.9927612085942906,\n",
      "    'train_f1': 0.9927751005396068,\n",
      "    'train_loss': 0.02563690021634102}\n",
      "[   (   'SELECT COUNT_DISTINCT(GAME_NAME) FROM t1',\n",
      "        ('Find the Distinct count of games for RPG genre', 'GAME_NAME')),\n",
      "    (   'SELECT COUNT_DISTINCT(GAME_NAME) FROM t1',\n",
      "        (   'Find the distinct count of games for RPG and Adventure genre',\n",
      "            'GAME_NAME')),\n",
      "    (   'SELECT COUNT_DISTINCT(TITLE_ID) FROM t1',\n",
      "        (   'Find the number of distinct Title_ID for Horizon Zero Dawn',\n",
      "            'TITLE_ID')),\n",
      "    (   'SELECT COUNT(GAME_NAME) FROM t1',\n",
      "        ('Find the Number of games under franchise 125690', 'GAME_NAME_ID')),\n",
      "    (   'SELECT SUM(SESSION_LENGTH_SECONDS) FROM t1',\n",
      "        (   'FIND TOTAL PLAYTIME FOR GAME_NAME_ID = 123456',\n",
      "            'PRIMARY_SESSION_LENGTH_SECONDS')),\n",
      "    (   'SELECT COUNT(PRIMARY_account_ID) FROM t1',\n",
      "        (   'FIND TOTAL NUMBER OF ACCOUNTS WHO PLAYED GAME_NAME_ID = 123456',\n",
      "            'PRIMARY_account_ID')),\n",
      "    (   'SELECT COUNT(PRIMARY_account_ID) FROM t1',\n",
      "        (   'FIND TOTAL NUMBER OF ACCOUNTS WHO PLAYED ON DATE = 11/20/2018',\n",
      "            'PRIMARY_account_ID')),\n",
      "    (   'SELECT SUM(PSN_TENURE_DAYS) FROM t1',\n",
      "        (   'Find total revenue for the title whose title_id=118444 in US.  ',\n",
      "            'SPEND_USD_CENTS')),\n",
      "    (   'SELECT COUNT_DISTINCT(account_ID) FROM t1',\n",
      "        ('Find number of accounts in region 4 ', 'account_ID')),\n",
      "    (   'SELECT SUM(SPEND_USD_CENTS) FROM t1',\n",
      "        ('Find total sales for the title whose title_id=118444', 'QUANTITY')),\n",
      "    (   'SELECT COUNT(account_ID) FROM t1',\n",
      "        (   'Count number of accounts of who made transaction on 11/26/2018.',\n",
      "            'account_ID')),\n",
      "    (   'SELECT COUNT_DISTINCT(TROPHY_ID) FROM t1',\n",
      "        (   'How many distinct trophy ids are there for trophy name \"Battle '\n",
      "            'for Control\"',\n",
      "            'TROPHY_ID')),\n",
      "    (   'SELECT (TROPHY_PID) FROM t1',\n",
      "        (   \"List of trophies that are marked under title '13333'\",\n",
      "            'TROPHY_NAME')),\n",
      "    (   'SELECT (TROPHY_TYPE) FROM t1',\n",
      "        (   'What is the trophy_type of trophy name \"The Ballerina\"',\n",
      "            'TROPHY_TYPE')),\n",
      "    (   'SELECT COUNT(FRANCHISE_NAME) FROM t1',\n",
      "        (   'How many franchisee names are present for franchisee name id '\n",
      "            '75230',\n",
      "            'FRANCHISE_NAME')),\n",
      "    (   'SELECT COUNT(GAME_NAME) FROM t1',\n",
      "        (   'How many Game names are present for franchisee name id 75230',\n",
      "            'GAME_NAME')),\n",
      "    (   'SELECT COUNT(FRANCHISE_NAME) FROM t1',\n",
      "        (   'How many franchisee names are present for Party Type 3rd Party',\n",
      "            'FRANCHISE_NAME')),\n",
      "    (   'SELECT (GAME_GENRE) FROM t1',\n",
      "        (   'What is/are the game genre(s) for franchisee name id 75230',\n",
      "            'GAME_GENRE'))]\n",
      " Loss: 0.04055002081493678, batch: 880\n",
      "\n",
      "{   'epoch': 10,\n",
      "    'test_accuracy': 0.930630744051366,\n",
      "    'test_f1': 0.930658305140159,\n",
      "    'test_loss': 0.43554484844207764,\n",
      "    'train_accuracy': 0.993417667618828,\n",
      "    'train_f1': 0.9934471597254527,\n",
      "    'train_loss': 0.022381749004125595}\n",
      "[   (   'SELECT COUNT_DISTINCT(GAME_NAME) FROM t1',\n",
      "        ('Find the Distinct count of games for RPG genre', 'GAME_NAME')),\n",
      "    (   'SELECT COUNT_DISTINCT(GAME_NAME) FROM t1',\n",
      "        (   'Find the distinct count of games for RPG and Adventure genre',\n",
      "            'GAME_NAME')),\n",
      "    (   'SELECT COUNT_DISTINCT(TITLE_ID) FROM t1',\n",
      "        (   'Find the number of distinct Title_ID for Horizon Zero Dawn',\n",
      "            'TITLE_ID')),\n",
      "    (   'SELECT COUNT(GAME_NAME) FROM t1',\n",
      "        ('Find the Number of games under franchise 125690', 'GAME_NAME_ID')),\n",
      "    (   'SELECT SUM(PRIMARY_SESSION_LENGTH_SECONDS) FROM t1',\n",
      "        (   'FIND TOTAL PLAYTIME FOR GAME_NAME_ID = 123456',\n",
      "            'PRIMARY_SESSION_LENGTH_SECONDS')),\n",
      "    (   'SELECT COUNT(PRIMARY_account_ID) FROM t1',\n",
      "        (   'FIND TOTAL NUMBER OF ACCOUNTS WHO PLAYED GAME_NAME_ID = 123456',\n",
      "            'PRIMARY_account_ID')),\n",
      "    (   'SELECT COUNT(PRIMARY_account_ID) FROM t1',\n",
      "        (   'FIND TOTAL NUMBER OF ACCOUNTS WHO PLAYED ON DATE = 11/20/2018',\n",
      "            'PRIMARY_account_ID')),\n",
      "    (   'SELECT SUM(TITLE_ID) FROM t1',\n",
      "        (   'Find total revenue for the title whose title_id=118444 in US.  ',\n",
      "            'SPEND_USD_CENTS')),\n",
      "    (   'SELECT COUNT_DISTINCT(account_ID) FROM t1',\n",
      "        ('Find number of accounts in region 4 ', 'account_ID')),\n",
      "    (   'SELECT SUM(PSN_TENURE_DAYS) FROM t1',\n",
      "        ('Find total sales for the title whose title_id=118444', 'QUANTITY')),\n",
      "    (   'SELECT COUNT(account_ID) FROM t1',\n",
      "        (   'Count number of accounts of who made transaction on 11/26/2018.',\n",
      "            'account_ID')),\n",
      "    (   'SELECT COUNT_DISTINCT(TROPHY_ID) FROM t1',\n",
      "        (   'How many distinct trophy ids are there for trophy name \"Battle '\n",
      "            'for Control\"',\n",
      "            'TROPHY_ID')),\n",
      "    (   'SELECT (TROPHY_PID) FROM t1',\n",
      "        (   \"List of trophies that are marked under title '13333'\",\n",
      "            'TROPHY_NAME')),\n",
      "    (   'SELECT (TROPHY_TYPE) FROM t1',\n",
      "        (   'What is the trophy_type of trophy name \"The Ballerina\"',\n",
      "            'TROPHY_TYPE')),\n",
      "    (   'SELECT COUNT(FRANCHISE_NAME) FROM t1',\n",
      "        (   'How many franchisee names are present for franchisee name id '\n",
      "            '75230',\n",
      "            'FRANCHISE_NAME')),\n",
      "    (   'SELECT COUNT(GAME_NAME) FROM t1',\n",
      "        (   'How many Game names are present for franchisee name id 75230',\n",
      "            'GAME_NAME')),\n",
      "    (   'SELECT COUNT(FRANCHISE_NAME) FROM t1',\n",
      "        (   'How many franchisee names are present for Party Type 3rd Party',\n",
      "            'FRANCHISE_NAME')),\n",
      "    (   'SELECT (GAME_GENRE_ID) FROM t1',\n",
      "        (   'What is/are the game genre(s) for franchisee name id 75230',\n",
      "            'GAME_GENRE'))]\n",
      " Loss: 0.033653169181409544, batch: 880\n",
      "\n",
      "{   'epoch': 11,\n",
      "    'test_accuracy': 0.9301271559864032,\n",
      "    'test_f1': 0.930140153484357,\n",
      "    'test_loss': 0.456809937953949,\n",
      "    'train_accuracy': 0.993825736742189,\n",
      "    'train_f1': 0.9938420981683744,\n",
      "    'train_loss': 0.020190076902508736}\n",
      "[   (   'SELECT COUNT_DISTINCT(GAME_NAME) FROM t1',\n",
      "        ('Find the Distinct count of games for RPG genre', 'GAME_NAME')),\n",
      "    (   'SELECT COUNT_DISTINCT(GAME_NAME) FROM t1',\n",
      "        (   'Find the distinct count of games for RPG and Adventure genre',\n",
      "            'GAME_NAME')),\n",
      "    (   'SELECT COUNT_DISTINCT(TITLE_ID) FROM t1',\n",
      "        (   'Find the number of distinct Title_ID for Horizon Zero Dawn',\n",
      "            'TITLE_ID')),\n",
      "    (   'SELECT COUNT(GAME_NAME) FROM t1',\n",
      "        ('Find the Number of games under franchise 125690', 'GAME_NAME_ID')),\n",
      "    (   'SELECT SUM(PRIMARY_SESSION_LENGTH_SECONDS) FROM t1',\n",
      "        (   'FIND TOTAL PLAYTIME FOR GAME_NAME_ID = 123456',\n",
      "            'PRIMARY_SESSION_LENGTH_SECONDS')),\n",
      "    (   'SELECT COUNT(PRIMARY_account_ID) FROM t1',\n",
      "        (   'FIND TOTAL NUMBER OF ACCOUNTS WHO PLAYED GAME_NAME_ID = 123456',\n",
      "            'PRIMARY_account_ID')),\n",
      "    (   'SELECT COUNT(PRIMARY_account_ID) FROM t1',\n",
      "        (   'FIND TOTAL NUMBER OF ACCOUNTS WHO PLAYED ON DATE = 11/20/2018',\n",
      "            'PRIMARY_account_ID')),\n",
      "    (   'SELECT SUM(PSN_TENURE_DAYS) FROM t1',\n",
      "        (   'Find total revenue for the title whose title_id=118444 in US.  ',\n",
      "            'SPEND_USD_CENTS')),\n",
      "    (   'SELECT COUNT_DISTINCT(account_ID) FROM t1',\n",
      "        ('Find number of accounts in region 4 ', 'account_ID')),\n",
      "    (   'SELECT SUM(PSN_TENURE_DAYS) FROM t1',\n",
      "        ('Find total sales for the title whose title_id=118444', 'QUANTITY')),\n",
      "    (   'SELECT COUNT(account_ID) FROM t1',\n",
      "        (   'Count number of accounts of who made transaction on 11/26/2018.',\n",
      "            'account_ID')),\n",
      "    (   'SELECT COUNT_DISTINCT(TROPHY_ID) FROM t1',\n",
      "        (   'How many distinct trophy ids are there for trophy name \"Battle '\n",
      "            'for Control\"',\n",
      "            'TROPHY_ID')),\n",
      "    (   'SELECT (TROPHY_PID) FROM t1',\n",
      "        (   \"List of trophies that are marked under title '13333'\",\n",
      "            'TROPHY_NAME')),\n",
      "    (   'SELECT (TROPHY_TYPE) FROM t1',\n",
      "        (   'What is the trophy_type of trophy name \"The Ballerina\"',\n",
      "            'TROPHY_TYPE')),\n",
      "    (   'SELECT COUNT(FRANCHISE_NAME) FROM t1',\n",
      "        (   'How many franchisee names are present for franchisee name id '\n",
      "            '75230',\n",
      "            'FRANCHISE_NAME')),\n",
      "    (   'SELECT COUNT(GAME_NAME) FROM t1',\n",
      "        (   'How many Game names are present for franchisee name id 75230',\n",
      "            'GAME_NAME')),\n",
      "    (   'SELECT COUNT(FRANCHISE_NAME) FROM t1',\n",
      "        (   'How many franchisee names are present for Party Type 3rd Party',\n",
      "            'FRANCHISE_NAME')),\n",
      "    (   'SELECT (GAME_GENRE) FROM t1',\n",
      "        (   'What is/are the game genre(s) for franchisee name id 75230',\n",
      "            'GAME_GENRE'))]\n",
      " Loss: 0.03387828770844149, batch: 783"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-135-44c20fe9bbfa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    sel_nn.train()\n",
    "    loss_value = 0\n",
    "    for batch_no, (X, y) in enumerate(train_dataloader):\n",
    "#         print(y.shape)\n",
    "        y, _ = y\n",
    "        y_pred, _ = sel_nn(*X)\n",
    "#         _, y = y\n",
    "#         _, y_pred = sel_nn(*X)\n",
    "\n",
    "        _, y_true_label = torch.max(y, 1)\n",
    "        loss = loss_function(y_pred, y_true_label.long())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_value += loss.item()\n",
    "        print('\\r', \"Loss: {}, batch: {}\".format(loss_value / (batch_no if batch_no > 0 else 1), batch_no), end=\"\")\n",
    "    #         break\n",
    "    #     break\n",
    "\n",
    "    #     print('fc2:',  sel_nn.*fc2.weight)\n",
    "    torch.save(sel_nn, 'data/training/SelAggTrainableEmb{epoch}.pt'.format(epoch=i))\n",
    "    with torch.no_grad():\n",
    "        sel_nn.eval()\n",
    "\n",
    "        train_pred = []\n",
    "        train_true = []\n",
    "        train_pred_pd = []  # , torch.tensor([], dtype=torch.float)\n",
    "        train_true_pd = []  # , torch.tensor([], dtype=torch.float)\n",
    "\n",
    "        for X, y in train_dataloader:\n",
    "            y, _ = y\n",
    "            y_pred_train, _ = sel_nn(*X)\n",
    "#             _, y = y\n",
    "#             _, y_pred_train = sel_nn(*X)\n",
    "            _, y_pred_train_label = torch.max(y_pred_train, 1)\n",
    "            _, y_true_train_label = torch.max(y, 1)\n",
    "\n",
    "            #             train_pred_pd = torch.cat((train_pred_pd, y_pred_train.cpu()))\n",
    "            #             train_true_pd = torch.cat((train_true_pd, y.cpu()))\n",
    "            train_pred_pd += y_pred_train.tolist()\n",
    "            train_true_pd += y.tolist()\n",
    "\n",
    "            train_pred += y_pred_train_label.tolist()\n",
    "            train_true += y_true_train_label.tolist()\n",
    "\n",
    "        test_pred = []\n",
    "        test_true = []\n",
    "        #         test_pred_pd = torch.tensor([], dtype=torch.float)\n",
    "        #         test_true_pd = torch.tensor([], dtype=torch.float)\n",
    "        test_pred_pd = []\n",
    "        test_true_pd = []\n",
    "        for X, y in test_dataloader:\n",
    "            y, _ = y\n",
    "            y_pred_test, _ = sel_nn(*X)\n",
    "#             _, y = y\n",
    "#             _, y_pred_test = sel_nn(*X)\n",
    "            _, y_pred_test_label = torch.max(y_pred_test, 1)\n",
    "            _, y_true_test_label = torch.max(y, 1)\n",
    "\n",
    "            #             test_pred_pd = torch.cat((test_pred_pd, y_pred_test.cpu()))\n",
    "            #             test_true_pd = torch.cat((test_true_pd, y.cpu()))\n",
    "\n",
    "            test_pred_pd += y_pred_test.tolist()\n",
    "            test_true_pd += y.tolist()\n",
    "\n",
    "            test_pred += y_pred_test_label.tolist()\n",
    "            test_true += y_true_test_label.tolist()\n",
    "\n",
    "        print('\\n')\n",
    "\n",
    "        train_pred_pd = torch.tensor(train_pred_pd, device=device)\n",
    "        test_pred_pd = torch.tensor(test_pred_pd, device=device)\n",
    "\n",
    "        train_true_pd = torch.tensor(train_true_pd, device=device)\n",
    "        test_true_pd = torch.tensor(test_true_pd, device=device)\n",
    "\n",
    "        _, train_true_label = torch.max(train_true_pd, 1)\n",
    "        _, test_true_label = torch.max(test_true_pd, 1)\n",
    "\n",
    "        train_true_label = train_true_label.long()\n",
    "        test_true_label = test_true_label.long()\n",
    "\n",
    "        performance = {'epoch': i,\n",
    "                       'train_loss': loss_function(train_pred_pd.cuda(), train_true_label.cuda()).item(),\n",
    "                       'test_loss': loss_function(test_pred_pd.cuda(), test_true_label.cuda()).item(),\n",
    "                       'train_accuracy': accuracy_score(train_true, train_pred),\n",
    "                       'test_accuracy': accuracy_score(test_true, test_pred),\n",
    "                       'train_f1': f1_score(train_true, train_pred,\n",
    "                                            average='weighted'),\n",
    "                       'test_f1': f1_score(test_true, test_pred,\n",
    "                                           average='weighted')}\n",
    "\n",
    "        pprint.pprint(performance, indent=4)\n",
    "        pprint.pprint(performance, open('data/trainAttention.log', 'a+'), indent=4)\n",
    "        pp.pprint(print_sony())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sel_nn = torch.load('nl2sql/trained/AggSelPredictorEpoch8.pt')\n",
    "# sel_nn = torch.load(\"nl2sql/trained/SelAggPredictorWOWeightsEpoch5.pt\")\n",
    "# sel_nn = torch.load(\"data/training/SelTrainableEmb7.pt\")\n",
    "sel_nn = torch.load('data/training/SelAggTrainableEmb2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/project/anaconda3/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type SelectPredictor. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(sel_nn, 'nl2sql/trained/AggSelPredictor2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6658751528846647"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spatial.distance.cosine(datatransformer.embedding_matrix[35], datatransformer.embedding_matrix[135])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datatransformer.transform_questions(['what is the longest'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datatransform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datatransformer.tokenizer.word_index['game']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# nlp = spacy.load('en')\n",
    "from nl2sql.nl2sql.predictor import Predictor\n",
    "def sub_cols(xs):\n",
    "    xs = list(map(lambda x: re.sub('ACCT', 'account', x, flags=re.IGNORECASE), xs))\n",
    "    return xs\n",
    "data_ = list(zip(sony_df['question'].values, sony_df.apply(lambda x: sub_cols(eval(x['header']))[x['sel_col']], axis=1).values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sony():\n",
    "\n",
    "    def format_sql(sels, aggs):\n",
    "        sql = \"SELECT {agg}({col}) FROM t1\"\n",
    "        return [sql.format(agg=agg, col=sel) for agg, sel in zip(aggs, sels)]\n",
    "    \n",
    "    \n",
    "    def sub_find(x):\n",
    "        \n",
    "#         x = re.sub('find', 'what is', x, flags=re.IGNORECASE)\n",
    "#         x = re.sub('account', 'account', x, flags=re.IGNORECASE)\n",
    "#         x = re.sub('id', 'id', x, flags=re.IGNORECASE)\n",
    "        x = re.sub('franchisee', 'franchise', x, flags=re.IGNORECASE)\n",
    "        return x\n",
    "\n",
    "    \n",
    "\n",
    "    predictor = Predictor(sel_nn, datatransformer)\n",
    "    preds = predictor.predict_selcol(sony_df.question.apply(sub_find).values, \n",
    "                        sony_df.header.apply(lambda x: sub_cols(eval(x))).values)\n",
    "    return list(zip(format_sql(*preds), data_))\n",
    "# pp.pprint(print_sony())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = Predictor(sel_nn, datatransformer)\n",
    "def format_sql(sels, aggs):\n",
    "    sql = \"SELECT {agg}({col}) FROM t1\"\n",
    "    return [sql.format(agg=agg, col=sel) for agg, sel in zip(aggs, sels)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/project/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:144: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/home/project/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:60: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/home/project/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:126: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['SELECT MAX(salary) FROM t1',\n",
       " 'SELECT COUNT(salary) FROM t1',\n",
       " 'SELECT (fighter) FROM t1',\n",
       " 'SELECT COUNT_DISTINCT(currency) FROM t1',\n",
       " 'SELECT AVG(salary) FROM t1',\n",
       " 'SELECT MIN(no) FROM t1',\n",
       " 'SELECT MAX(relative atomic mass) FROM t1']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = predictor.predict_selcol(['Who is the highest paid employee ?',\n",
    "                                  'How many employees are paid greater than 1000 ?', \n",
    "                          'who are the fighter with no losses in wwe ?',\n",
    "                          'how many distinct number of currency are used in Australia ? ',\n",
    "                          'what is the average pay?',\n",
    "                          'what is the lowest ctc ?',\n",
    "                         'what is the highest electrons?'], \n",
    "                    [['ID', 'salary', 'age', 'sex', 'designation', 'date of joining', 'address'],\n",
    "                     ['ID', 'salary', 'age', 'sex', 'designation', 'date of joining', 'address'],\n",
    "                     ['fighter', 'weight', 'wins', 'losses', 'origin'],\n",
    "                    ['currency', 'US dollar value', 'country'],\n",
    "                    ['no', 'salary', 'age', 'sex', 'designation', 'date of joining', 'address'],\n",
    "                    ['no', 'salary', 'age', 'sex', 'designation', 'date of joining', 'address'],\n",
    "                    ['id', 'element', 'relative atomic mass', 'atomic number']])\n",
    "format_sql(*preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(sel_nn.state_dict(), \"data/AggSelPredictorState.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
